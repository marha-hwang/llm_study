{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65aa2ee-72d6-4b6e-ad39-e39bda586698",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Similarity and Dot Product in Machine Learning\n",
    "\n",
    "- https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de\n",
    "- https://medium.com/data-science-collective/cosine-distance-vs-dot-product-vs-euclidean-in-vector-similarity-search-227a6db32edb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9484322-7709-4517-b993-3463aa1520fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Introduction to Vectors in ML\n",
    "\n",
    "Vectors as Data Representations: In ML, data (e.g., words, documents, images) is encoded as vectors in high-dimensional space.\n",
    "\n",
    "Example: TF-IDF vectors for documents.\n",
    "\n",
    "- Doc A: [10, 20, 30]\n",
    "- Doc B: [100, 200, 300] (scaled version of A).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why Similarity?: Measures how \"related\" two data points are. Applications:\n",
    "\n",
    "- Information retrieval (search engines).\n",
    "- Recommendation systems.\n",
    "- Natural Language Processing (NLP).\n",
    "\n",
    "\n",
    "Key Insight: Similarity can focus on direction (angle) or magnitude (length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31950484-89ff-4377-b604-24d4e417c23d",
   "metadata": {},
   "source": [
    "<img src='imgs/fig_embedding_space.png' width=800>\n",
    "<a href='https://miro.medium.com/v2/resize:fit:2000/format:webp/1*vOvIYq7cLw4qTC70tSwwgQ.png'>Figure 1: Embedding Space </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7819f8-533d-4da4-b273-991e84522a76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dot Product: The Foundation\n",
    "\n",
    "- Algebraic Definition:\n",
    "For vectors $\\vec{a} = [a_1, \\dots, a_n]$ and $\\vec{b} = [b_1, \\dots, b_n]$,\n",
    "$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^n a_i b_i$\n",
    "- Geometric Interpretation:\n",
    "$\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos \\theta$\n",
    "where $\\|\\vec{a}\\| = \\sqrt{\\sum a_i^2}$ is the Euclidean norm, and $\\theta$ is the angle between vectors.\n",
    "\n",
    "- Properties:\n",
    "    - Captures both direction and magnitude.\n",
    "    - Range: $-\\infty$ to $+\\infty$.\n",
    "    - Positive: Acute angle (similar direction).\n",
    "    - Zero: Perpendicular (orthogonal).\n",
    "    - Negative: Obtuse angle (opposite).\n",
    "\n",
    "\n",
    "- Limitation: Sensitive to vector length → Longer vectors yield higher scores (e.g., long documents dominate in search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a007c1-0c61-4b76-a1df-b6722bf9dea9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cosine Similarity: Direction-Only Measure\n",
    "\n",
    "- Formula:\n",
    "$\\text{Cosine Similarity} = \\cos \\theta = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}$\n",
    "\n",
    "- Key Advantage: Normalizes for magnitude → Focuses purely on angle.\n",
    "- Range: -1 (exactly opposite) to +1 (identical direction); 0 (orthogonal).\n",
    "- Example Calculation:\n",
    "    - $\\vec{a} = [1.5, 1.5]$, $\\vec{b} = [2.0, 1.0]$\n",
    "    - Dot product: $1.5 \\cdot 2.0 + 1.5 \\cdot 1.0 = 4.5$\n",
    "    - Norms: $\\|\\vec{a}\\| \\approx 2.121$, $\\|\\vec{b}\\| \\approx 2.236$\n",
    "    - Cosine: $4.5 / (2.121 \\cdot 2.236) \\approx 0.949$ (close to 1, similar direction).\n",
    "\n",
    "- Use Cases: Text similarity (ignores document length), word embeddings (e.g., Word2Vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fce31-488e-4764-9909-faa2898f9ed3",
   "metadata": {},
   "source": [
    "<img src='imgs/fig_cos_sim.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809e3be-c88f-4565-b0fc-06d862517a38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Proof of the Cosine Rule (Geometric Derivation)\n",
    "\n",
    "- Setup: Treat $\\vec{a}$ and $\\vec{b}$ as adjacent sides of a triangle. The third side is $\\vec{a} - \\vec{b}$.\n",
    "- Law of Cosines (from geometry):\n",
    "$\\|\\vec{a} - \\vec{b}\\|^2 = \\|\\vec{a}\\|^2 + \\|\\vec{b}\\|^2 - 2 \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos \\theta$\n",
    "- Expand the Left Side:\n",
    "$\\|\\vec{a} - \\vec{b}\\|^2 = (\\vec{a} - \\vec{b}) \\cdot (\\vec{a} - \\vec{b}) = \\vec{a} \\cdot \\vec{a} - 2 \\vec{a} \\cdot \\vec{b} + \\vec{b} \\cdot \\vec{b} = \\|\\vec{a}\\|^2 - 2 \\vec{a} \\cdot \\vec{b} + \\|\\vec{b}\\|^2$\n",
    "- Equate Both Sides:\n",
    "$\\|\\vec{a}\\|^2 - 2 \\vec{a} \\cdot \\vec{b} + \\|\\vec{b}\\|^2 = \\|\\vec{a}\\|^2 + \\|\\vec{b}\\|^2 - 2 \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos \\theta$\n",
    "- Simplify:\n",
    "$-2 \\vec{a} \\cdot \\vec{b} = -2 \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos \\theta$\n",
    "$\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos \\theta$,\n",
    "$\\cos \\theta = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}$\n",
    "- Validity: Proven in 2D/3D via Pythagorean theorem; generalizes to n-dimensions as dot product and norms are defined identically.\n",
    "- Note: Assumes real vectors; holds for Euclidean space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b66c4cd-f0bb-4be5-ba43-f21fbc51b565",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1.0, 0.0])  # Unit vector\n",
    "b = np.array([0.0, 1.0])  # Unit vector, orthogonal\n",
    "\n",
    "cos_theta = np.dot(a, b)  # 0.0\n",
    "d = np.linalg.norm(a - b)  # sqrt(2) ≈ 1.414\n",
    "print(np.sqrt(2 - 2 * cos_theta))  # Matches: 1.414"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8fbda-13a3-4327-b4b3-29a0e6b096b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Euclidean Distance: Magnitude and Direction\n",
    "\n",
    "- Formula:\n",
    "$d(\\vec{a}, \\vec{b}) = \\|\\vec{a} - \\vec{b}\\| = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}$\n",
    "- Relation to Cosine: For unit-norm vectors, $d = \\sqrt{2 - 2 \\cos \\theta}$.\n",
    "- Properties:\n",
    "\n",
    "    - Always non-negative (0 for identical vectors).\n",
    "    - Satisfies triangle inequality → Useful for indexing (e.g., KD-trees).\n",
    "\n",
    "\n",
    "- When to Use: Clustering (k-means), anomaly detection → Considers absolute differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ef490-bbc8-4ba0-a379-01d1298a1bcf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Comparison of Measures\n",
    "\n",
    "| Measure            | Formula                                      | Magnitude-Sensitive? | Range      | Best For                          | Is a Metric?                       |\n",
    "|--------------------|----------------------------------------------|----------------------|------------|-----------------------------------|------------------------------------|\n",
    "| Dot Product        | $\\vec{a} \\cdot \\vec{b}$                      | Yes                  | $-\\infty$ to $\\infty$ | Fast similarity, inner prod.      | No                                 |\n",
    "| Cosine Similarity  | $\\dfrac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\|\\;\\|\\vec{b}\\|}$ | No                   | $-1$ to $1$ | Directional (text, embeddings)    | No (but $1 - \\cos$ is distance)    |\n",
    "| Euclidean Distance | $\\sqrt{\\sum_i (a_i - b_i)^2}$                | Yes                  | $0$ to $\\infty$ | Absolute distance, clustering | Yes                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa702127-34b8-4d2b-a5d4-381a2dcbf0c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<center><img src='imgs/fig_dot_and_cos.png' width=800></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440db1ec-efa5-4d8f-8ade-e8c79bee131e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot: 14000\n",
      "Cosine: 1.0\n",
      "Euclidean: 336.7491648096547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dot_product(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return np.dot(a, b) / (norm_a * norm_b)\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# Example\n",
    "a = np.array([10, 20, 30])\n",
    "b = np.array([100, 200, 300])\n",
    "\n",
    "print(\"Dot:\", dot_product(a, b))         \n",
    "print(\"Cosine:\", cosine_similarity(a, b))\n",
    "print(\"Euclidean:\", euclidean_distance(a, b))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a8cc0-30be-47ce-86d6-d967c06eaa73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Attention Mechanism: Dot Product in Transformers\n",
    "\n",
    "- Context: From \"Attention Is All You Need\" (Vaswani et al., 2017).\n",
    "- Scaled Dot-Product Attention:\n",
    "\n",
    "    - Inputs: Queries (Q), Keys (K), Values (V) — projections of embeddings.\n",
    "    - Similarity Scores: $QK^T$ (matrix of dot products).\n",
    "    - Scaled: Divide by $\\sqrt{d_k}$ (dimension of keys) to stabilize gradients.\n",
    "    - Full Formula:\n",
    "    $\\text{Attention}(Q, K, V) = softmax\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$\n",
    "\n",
    "\n",
    "- Why Dot Product?:\n",
    "\n",
    "    - Efficient computation (O(n²) but parallelizable).\n",
    "    - Measures alignment (like unnormalized cosine).\n",
    "    - For normalized Q/K: Equivalent to cosine.\n",
    "\n",
    "\n",
    "- Intuition: High dot score → Query \"attends\" strongly to that Key → Weights Values accordingly.\n",
    "- Multi-Head Attention: Run in parallel heads → Capture different relationships.\n",
    "- Example: In translation, query \"it\" attends to subject in source sentence.\n",
    "- Scaling Rationale: In high dims, dot products grow large → Softmax saturates (gradients → 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2cda6-24ef-4b8f-86fc-ae295c84865c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
