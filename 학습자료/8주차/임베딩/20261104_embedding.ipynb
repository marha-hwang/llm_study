{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36f77f8-34a1-4caa-a97e-d44d4a89f3be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Word Embeddings in NLP\n",
    "\n",
    "- https://www.geeksforgeeks.org/nlp/word-embeddings-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546b463-a2cf-47fd-9f37-1853a4dde44b",
   "metadata": {},
   "source": [
    "Word Embeddings are numeric representations of words in a lower-dimensional space, that capture semantic and syntactic information. They play a important role in Natural Language Processing (NLP) tasks. Here, we'll discuss some traditional and neural approaches used to implement Word Embeddings, such as TF-IDF, Word2Vec, and GloVe.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20250530170636975663/Embeddings-in-Natural-Language-Processing.webp' width=600>\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20250531165319378386/Word-Embedding.webp' width=600>\n",
    "\n",
    "Above images represent the Process and an Example of Word Embeddings in Natural Language Processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda725b-2dfd-43a0-b6fd-f920b2ba59eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## What is Word Embedding in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81526bc-2871-4017-8d87-a1c5ab1bdf3a",
   "metadata": {},
   "source": [
    "Word Embedding is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space.\n",
    "\n",
    "- Method of extracting features out of text so that we can input those features into a machine learning model to work with text data.\n",
    "- It allows words with similar meanings to have a similar representation.\n",
    "- Thus, Similarity can be assessed based on Similar vector representations.\n",
    "- High Computation Cost: Large input vectors will mean a huge number of weights. Embeddings help to reduce dimensionality.\n",
    "- Preserve syntactical and semantic information.\n",
    "- Some methods based on Word Frequency are Bag of Words (BOW), Count Vectorizer and TF-IDF.\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/\n",
    "- https://www.geeksforgeeks.org/nlp/using-countvectorizer-to-extracting-features-from-text/\n",
    "- https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11301a7-461a-4bc2-b5c6-6f9379892ead",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Need for Word Embedding?\n",
    "- To reduce dimensionality.\n",
    "- To use a word to predict the words around it.\n",
    "- Helps in enhancing model interpretability due to numerical representation.\n",
    "- Inter-word semantics and similarity can be captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484507d-8259-4760-a626-af4b3f88e051",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## How are Word Embeddings used?\n",
    "\n",
    "- They are used as input to machine learning models.\n",
    "\n",
    "> `Words ----> Numeric representation ----> Use in Training or Inference.`\n",
    "\n",
    "- To represent or visualize any underlying patterns of usage in the corpus that was used to train them.\n",
    "\n",
    "Let's take an example to understand how word vector is generated by taking emotions which are most frequently used in certain conditions and transform each emoji into a vector and the conditions will be our features.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20240104185157/emoji.png' width=600>\n",
    "In a similar way, we can create word vectors for different words as well on the basis of given features. The words with similar vectors are most likely to have the same meaning or are used to convey the same sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c00f43-f3e0-4149-8332-0aa88e3e4c63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Approaches for Text Representation\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20250530170636672412/Techniques-in-Embeddings-in-NLP.webp' width=600>\n",
    "Techniques in Embeddings in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5782950-7044-4480-b942-f69703cd4fd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 1. Traditional Approach\n",
    "\n",
    "The conventional method involves compiling a list of distinct terms and giving each one a unique integer value or id, and after that, insert each word's distinct id into the sentence. Every vocabulary word is handled as a feature in this instance. Thus, a large vocabulary will result in an extremely large feature size. Common traditional methods include:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f07b20-c738-4a4a-97dc-21dd50485464",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### 1.1 One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1.\n",
    "\n",
    "Following are the disadvantages:\n",
    "\n",
    "- High-dimensional vectors, Computationally expensive and Memory-intensive\n",
    "- Does not capture Semantic Relationships\n",
    "- Restricted to the seen training vocabulary\n",
    "\n",
    "* https://www.geeksforgeeks.org/machine-learning/ml-one-hot-encoding/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7d722a-d070-441e-9151-38f27182946d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'mat', 'on', 'cat', 'tree', 'in', 'bird', 'hat', 'dog', 'the'}\n",
      "Word to Index Mapping: {'mat': 0, 'on': 1, 'cat': 2, 'tree': 3, 'in': 4, 'bird': 5, 'hat': 6, 'dog': 7, 'the': 8}\n",
      "One-Hot Encoded Matrix:\n",
      "\n",
      "cat: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "in: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "hat: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "dog: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "on: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "mat: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "bird: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "in: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "tree: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(text):\n",
    "    words = text.split()\n",
    "    vocabulary = set(words)\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    one_hot_encoded = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[word]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    return one_hot_encoded, word_to_index, vocabulary\n",
    "\n",
    "example_text = \"cat in the hat dog on the mat bird in the tree\"\n",
    "\n",
    "one_hot_encoded, word_to_index, vocabulary = one_hot_encode(example_text)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "print(\"\")\n",
    "for word, encoding in zip(example_text.split(), one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11729416-3eaa-4480-869d-2a39fd698bf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Bag of Word (Bow)\n",
    "\n",
    "Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation. Limitations are as follows:\n",
    "\n",
    "Ignores the order of words in the document: Causes loss of sequential information and context\n",
    "Sparse representations make it Memory intensive: Many elements are zero resulting in Computational inefficiency with large datasets.\n",
    "\n",
    "- https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82890ef1-234f-45a0-9943-987cf4fb44d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"This is the first document.\", \n",
    "             \"This document is the second document.\",\n",
    "             \"And this is the third one.\", \n",
    "             \"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43b58b-04e1-47ab-b349-6e5f3866af49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 1.3 Term frequency-inverse document frequency (TF-IDF)\n",
    "\n",
    "Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:\n",
    "\n",
    "- Term Frequency (TF): Measures how often a term (word) appears in a document.\n",
    "- Inverse Document Frequency (IDF): Measures the importance of a term across a collection of documents.\n",
    "\n",
    "The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:\n",
    "\n",
    "> `TF−IDF(t,d,D)=TF(t,d)×IDF(t,D)`\n",
    "\n",
    "The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering.\n",
    "\n",
    "Steps are as follows:\n",
    "\n",
    "1. Define a set of sample documents.\n",
    "2. Use TfidfVectorizer to transform these documents into a TF-IDF matrix.\n",
    "3. Extract and print the TF-IDF values for each word in each document.\n",
    "4. This statistical measure helps assess the importance of words in a document relative to their frequency across a collection of documents,\n",
    "5. Helps in information retrieval and text analysis tasks.\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebb8fd7-7029-4372-a664-27c6d24c19b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "the: 0.6030226891555273\n",
      "quick: 0.30151134457776363\n",
      "brown: 0.30151134457776363\n",
      "fox: 0.30151134457776363\n",
      "jumps: 0.30151134457776363\n",
      "over: 0.30151134457776363\n",
      "lazy: 0.30151134457776363\n",
      "dog: 0.30151134457776363\n",
      "\n",
      "\n",
      "Document 2:\n",
      "journey: 0.3535533905932738\n",
      "of: 0.3535533905932738\n",
      "thousand: 0.3535533905932738\n",
      "miles: 0.3535533905932738\n",
      "begins: 0.3535533905932738\n",
      "with: 0.3535533905932738\n",
      "single: 0.3535533905932738\n",
      "step: 0.3535533905932738\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [ \"The quick brown fox jumps over the lazy dog.\",\n",
    "              \"A journey of a thousand miles begins with a single step.\" ]\n",
    "\n",
    "vectorizer = TfidfVectorizer() \n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = {}\n",
    "\n",
    "for doc_index, doc in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[doc_index, :].nonzero()[1]\n",
    "    tfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n",
    "    tfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values}\n",
    "\n",
    "for doc_index, values in tfidf_values.items():\n",
    "    print(f\"Document {doc_index + 1}:\")\n",
    "    for word, tfidf_value in values.items():\n",
    "        print(f\"{word}: {tfidf_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e096026e-f4fe-482d-b0cf-337fa4ac70ae",
   "metadata": {},
   "source": [
    "Some of the disadvantages of TF-IDF are:\n",
    "\n",
    "- Inability in Capturing context: Doesn't consider semantic relationships in words.\n",
    "- Sensitivity to Document Length: Longer documents have higher overall term frequencies, potentially biasing TF-IDF towards longer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4228bb-f6a6-4dd6-839d-3908cec1ad4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 2. Neural Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ca5dd-4594-4c16-80c1-128254743404",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec\n",
    "\n",
    "Word2Vec is a neural approach for generating word embeddings. It belongs to the family of neural word embedding techniques and specifically falls under the category of distributed representation models. It is a popular technique in natural language processing (NLP).\n",
    "\n",
    "- Represent words as continuous vector spaces.\n",
    "- Aim: Capture the semantic relationships between words by mapping them to high-dimensional vectors.\n",
    "- Words with similar meanings should have similar vector representations. Every word is assigned a vector. We start with either a random or one-hot vector.\n",
    "\n",
    "There are two neural embedding methods for Word2Vec: \n",
    "- Continuous Bag of Words (CBOW)\n",
    "- Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480c941-7a1d-4e17-af01-dfea3f7c8769",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Continuous Bag of Words(CBOW)\n",
    "\n",
    "Continuous Bag of Words (CBOW) is a type of neural network architecture used in the Word2Vec model. The primary objective of CBOW is to predict a target word based on its context, which consists of the surrounding words in a given window. Given a sequence of words in a context window, the model is trained to predict the target word at the center of the window.\n",
    "\n",
    "- Feedforward neural network with a single hidden layer.\n",
    "- The input layer, hidden layer, and output layer represent the context words, learned continuous vectors or embeddings, and the target word.\n",
    "- Useful for learning distributed representations of words in a continuous vector space.\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20230416175601/Screenshot-(27)-660.png' width=400>\n",
    "\n",
    "The hidden layer contains the continuous vector representations (word embeddings) of the input words.\n",
    "\n",
    "- The weights between the input layer and the hidden layer are learned during training.\n",
    "- The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space).\n",
    "\n",
    "- https://www.geeksforgeeks.org/nlp/continuous-bag-of-words-cbow-in-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a043097-9b95-4b01-9fef-da822f3be97d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0\n",
      "Epoch 11, Loss: 0\n",
      "Epoch 21, Loss: 0\n",
      "Epoch 31, Loss: 0\n",
      "Epoch 41, Loss: 0\n",
      "Epoch 51, Loss: 0\n",
      "Epoch 61, Loss: 0\n",
      "Epoch 71, Loss: 0\n",
      "Epoch 81, Loss: 0\n",
      "Epoch 91, Loss: 0\n",
      "\n",
      "Embedding for 'embeddings': [[ 1.5517266   0.5099207  -0.23332104 -0.81714803  1.9765431   0.2860804\n",
      "   1.2714254   1.0138086  -0.3610885  -0.33426276]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define CBOW model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_embeds = self.embeddings(context).sum(dim=1)\n",
    "        output = self.linear(context_embeds)\n",
    "        return output\n",
    "\n",
    "context_size = 2\n",
    "raw_text = \"word embeddings are awesome\"\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "\n",
    "for i in range(2, len(tokens) - 2):\n",
    "    context = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]]\n",
    "    target = word_to_index[tokens[i]]\n",
    "    data.append((torch.tensor(context), torch.tensor(target)))\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize CBOW model\n",
    "cbow_model = CBOWModel(vocab_size, embed_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(context)\n",
    "        loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:    \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage\n",
    "word_to_lookup = \"embeddings\"\n",
    "word_index = word_to_index[word_to_lookup]\n",
    "embedding = cbow_model.embeddings(torch.tensor([word_index]))\n",
    "print(f\"\\nEmbedding for '{word_to_lookup}': {embedding.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fedae4-9a66-40ea-928c-86796f86e799",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 2.3 Skip-Gram\n",
    "\n",
    "The Skip-Gram model learns distributed representations of words in a continuous vector space. The main objective of Skip-Gram is to predict context words (words surrounding a target word) given a target word. This is the opposite of the Continuous Bag of Words (CBOW) model, where the objective is to predict the target word based on its context. It is shown that this method produces more meaningful embeddings.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/Skip-gram-architecture-2.jpg' width=600>\n",
    "\n",
    "- Output: Trained vectors of each word after many iterations through the corpus.\n",
    "- Preserve syntactical or semantic information, Converted to lower dimensions.\n",
    "- Similar meaning (semantic info) vectors are placed close to each other in space.\n",
    "- vector_size parameter controls the dimensionality of the word vectors, and you can adjust other parameters such as window.\n",
    "\n",
    "> Note: Word2Vec models can perform better with larger datasets. If you have a large corpus, you might achieve more meaningful word embeddings.\n",
    "\n",
    "- https://www.geeksforgeeks.org/python/implement-your-own-word2vecskip-gram-model-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053b19a5-1a3a-4283-9a51-7609955e3aa8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from gensim) (7.4.1)\n",
      "Requirement already satisfied: wrapt in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
      "Downloading gensim-4.4.0-cp310-cp310-macosx_11_0_arm64.whl (24.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c32d2bd-01da-4aeb-8248-c156a9bcd7ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeonghyunpark/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jeonghyunpark/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'word': [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03\n",
      "  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03\n",
      " -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03\n",
      "  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04\n",
      " -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03\n",
      "  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03\n",
      "  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03\n",
      "  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03\n",
      "  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03\n",
      " -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03\n",
      " -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03\n",
      " -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03\n",
      " -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04\n",
      " -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03\n",
      "  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03\n",
      " -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03\n",
      " -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03\n",
      " -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03\n",
      "  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03\n",
      " -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03\n",
      " -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03\n",
      "  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03\n",
      " -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03\n",
      "  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03\n",
      "  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  \n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sample = \"Word embeddings are dense vector representations of words.\"\n",
    "tokenized_corpus = word_tokenize(sample.lower()) \n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus],\n",
    "                          vector_size=100,  \n",
    "                          window=5,         \n",
    "                          sg=1,             \n",
    "                          min_count=1,      \n",
    "                          workers=4)        \n",
    "\n",
    "# Training\n",
    "skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\n",
    "skipgram_model.save(\"skipgram_model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "vector_representation = loaded_model.wv['word']\n",
    "\n",
    "print(\"Vector representation of 'word':\", vector_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef0002-cf6e-4a60-81c9-9c65017856c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## 3. Pretrained Word-Embedding\n",
    "\n",
    "Pre-trained word embeddings are representations of words that are learned from large corpora and are made available for reuse in various Natural Language Processing (NLP) tasks. These embeddings capture semantic relationships between words, allowing the model to understand similarities and relationships between different words in a meaningful way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a0a46-3ddf-4473-8710-101475958607",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 3.1 GloVe\n",
    "\n",
    "GloVe is trained on global word co-occurrence statistics. It leverages the global context to create word embeddings that reflect the overall meaning of words based on their co-occurrence probabilities. this method, we take the corpus and iterate through it and get the co-occurrence of each word with other words in the corpus. We get a co-occurrence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on.\n",
    "\n",
    "Let's see how the matrix is created. Corpus:\n",
    "\n",
    "> It is a nice evening. \\\n",
    "> Good Evening! \\\n",
    "> Is it a nice evening?\n",
    "\n",
    "\n",
    "|           | it       | is         | a           | nice        | evening      | good |\n",
    "|------------|-----------|------------|--------------|--------------|--------------|------|\n",
    "| **it**     | 0         |            |              |              |              |      |\n",
    "| **is**     | 1 + 1     | 0          |              |              |              |      |\n",
    "| **a**      | 1/2 + 1   | 1 + 1/2     | 0            |              |              |      |\n",
    "| **nice**   | 1/3 + 1/2 | 1/2 + 1/3   | 1 + 1        | 0            |              |      |\n",
    "| **evening**| 1/4 + 1/3 | 1/3 + 1/4   | 1/2 + 1/2    | 1 + 1        | 0            |      |\n",
    "| **good**   | 0         | 0          | 0            | 0            | 1            | 0    |\n",
    "\n",
    "\n",
    "\n",
    "The upper half of the matrix will be a reflection of the lower half. We can consider a window frame as well to calculate the co-occurrences by shifting the frame till the end of the corpus. This helps gather information about the context in which the word is used.\n",
    "\n",
    "- Vectors for each word is assigned randomly.\n",
    "- Take two pairs of vectors and see closeness in space.\n",
    "- If they occur together more often or have a higher value in the co-occurrence matrix and are far apart in space then they are brought close to each other.\n",
    "- If they are close to each other but are rarely or not frequently used together then they are moved further apart in space.\n",
    "- Output: Vector space representation that approximates the information from the co-occurrence matrix.\n",
    "\n",
    "> You can refer to the library used in this approach: Gensim\n",
    "- https://www.geeksforgeeks.org/nlp/Glove-Word-Embedding-in-NLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297cc28b-693d-41a0-bb1a-fd7c58f86f0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.5% 48.5/66.0MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "Similarity between 'learn' and 'learning' using GloVe: 0.802\n",
      "Similarity between 'india' and 'indian' using GloVe: 0.865\n",
      "Similarity between 'fame' and 'famous' using GloVe: 0.589\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "glove_model = load('glove-wiki-gigaword-50')\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    similarity = glove_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeaf5ca-e0a9-4edc-b562-ea7a5a47b1c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 3.2 FastText\n",
    "\n",
    "Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. This approach is particularly useful for handling out-of-vocabulary words and capturing morphological variations.\n",
    "\n",
    "- https://www.geeksforgeeks.org/machine-learning/fasttext-working-and-implementation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b79b3f-00bf-4b14-abc6-a867980ca31c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.0% 105.1/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============------------------------------------] 29.4% 282.0/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================------------------] 64.8% 621.3/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================----------------] 69.3% 664.5/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================================---------------] 72.0% 689.6/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
      "Similarity between 'learn' and 'learning' using FastText: 0.642\n",
      "Similarity between 'india' and 'indian' using FastText: 0.708\n",
      "Similarity between 'fame' and 'famous' using FastText: 0.519\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") ## Load the pre-trained fastText model\n",
    "\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    similarity = fasttext_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a51a82e8-3fea-421a-b230-bfc0426eaad8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### 3.3 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT is a transformer-based model that learns contextualized embeddings for words. It considers the entire context of a word by considering both left and right contexts, resulting in embeddings that capture rich contextual information.\n",
    "\n",
    "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20231004120248/Blank-diagram-(1).jpeg' width=500>\n",
    "\n",
    "\n",
    "- https://www.geeksforgeeks.org/nlp/how-to-generate-word-embedding-using-bert/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a9946a-4662-4a59-be53-fa751d40f330",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained BERT model and tokenizer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    tokens = tokenizer(pair, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Extract embeddings for the [CLS] token\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    similarity = torch.nn.functional.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0)\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
