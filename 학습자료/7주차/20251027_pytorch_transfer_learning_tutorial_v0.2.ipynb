{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qstkxnTrJKnk"
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://docs.pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btJ8EwVTJKnm"
   },
   "source": [
    "Transfer Learning for Computer Vision Tutorial\n",
    "==============================================\n",
    "\n",
    "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)\n",
    "\n",
    "In this tutorial, you will learn how to train a convolutional neural\n",
    "network for image classification using transfer learning. You can read\n",
    "more about the transfer learning at [cs231n\n",
    "notes](https://cs231n.github.io/transfer-learning/)\n",
    "\n",
    "Quoting these notes,\n",
    "\n",
    "> In practice, very few people train an entire Convolutional Network\n",
    "> from scratch (with random initialization), because it is relatively\n",
    "> rare to have a dataset of sufficient size. Instead, it is common to\n",
    "> pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
    "> contains 1.2 million images with 1000 categories), and then use the\n",
    "> ConvNet either as an initialization or a fixed feature extractor for\n",
    "> the task of interest.\n",
    "\n",
    "These two major transfer learning scenarios look as follows:\n",
    "\n",
    "-   **Finetuning the ConvNet**: Instead of random initialization, we\n",
    "    initialize the network with a pretrained network, like the one that\n",
    "    is trained on imagenet 1000 dataset. Rest of the training looks as\n",
    "    usual.\n",
    "-   **ConvNet as fixed feature extractor**: Here, we will freeze the\n",
    "    weights for all of the network except that of the final fully\n",
    "    connected layer. This last fully connected layer is replaced with a\n",
    "    new one with random weights and only this layer is trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJTQfPR7JKnn"
   },
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "# Import required libraries for transfer learning\n",
    "import torch                          # PyTorch core\n",
    "import torch.nn as nn                 # Neural network modules\n",
    "import torch.optim as optim           # Optimization algorithms (SGD, Adam, etc.)\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "import torch.backends.cudnn as cudnn  # CUDA optimization\n",
    "import numpy as np                    # Numerical operations\n",
    "import torchvision                    # Computer vision utilities\n",
    "from torchvision import datasets, models, transforms  # Datasets, pretrained models, transforms\n",
    "import matplotlib.pyplot as plt       # Plotting\n",
    "import time                          # Timing training\n",
    "import os                            # File operations\n",
    "from PIL import Image                # Image loading\n",
    "from tempfile import TemporaryDirectory  # Temporary file storage\n",
    "\n",
    "# Enable cuDNN benchmarking for faster training (finds optimal algorithms)\n",
    "cudnn.benchmark = True\n",
    "# Enable interactive plotting mode\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard\n",
    "================\n",
    "\n",
    "- https://www.tensorflow.org/tensorboard/get_started\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir runs/transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup (optional)\n",
    "# TensorBoard provides visualization for training metrics, model graphs, and more\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    # Create a writer that logs to runs/transfer_learning directory\n",
    "    writer = SummaryWriter(log_dir=\"runs/transfer_learning\")\n",
    "    print(\"TensorBoard writer created: runs/transfer_learning\")\n",
    "    print(\"To launch: tensorboard --logdir runs/transfer_learning\")\n",
    "    print(\"Then open http://localhost:6006 in your browser\")\n",
    "except Exception as e:\n",
    "    writer = None\n",
    "    print(\"TensorBoard not available; logging disabled.\")\n",
    "    print(\"Reason:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/transfer_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsxYwWxZJKnn"
   },
   "source": [
    "Load Data\n",
    "=========\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data.\n",
    "\n",
    "The problem we\\'re going to solve today is to train a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants\n",
    "and bees. There are 75 validation images for each class. Usually, this\n",
    "is a very small dataset to generalize upon, if trained from scratch.\n",
    "Since we are using transfer learning, we should be able to generalize\n",
    "reasonably well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    "download : https://download.pytorch.org/tutorial/hymenoptera_data.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract hymenoptera_data without relying on shell tools\n",
    "# This works cross-platform (Windows, macOS, Linux) using Python's standard library\n",
    "import os, zipfile, urllib.request\n",
    "\n",
    "# Dataset URL and local paths\n",
    "url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
    "zip_path = \"hymenoptera_data.zip\"\n",
    "dst_root = \"data\"  # matches data_dir = 'data/hymenoptera_data'\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(dst_root, exist_ok=True)\n",
    "\n",
    "# Download dataset if not already present\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "else:\n",
    "    print(\"Zip file already exists, skipping download.\")\n",
    "\n",
    "# Extract to data/ directory so final path is data/hymenoptera_data/{train,val}\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    print(\"Extracting...\")\n",
    "    z.extractall(dst_root)\n",
    "\n",
    "# Verify extraction was successful\n",
    "expected_root = os.path.join(dst_root, \"hymenoptera_data\")\n",
    "print(\"Extracted to:\", expected_root)\n",
    "print(\"Train exists:\", os.path.isdir(os.path.join(expected_root, \"train\")))\n",
    "print(\"Val exists:\", os.path.isdir(os.path.join(expected_root, \"val\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMmqhMpfJKnn"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Training uses augmentation to prevent overfitting and improve generalization\n",
    "# Validation uses only normalization to get consistent evaluation results\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),      # Random crop to 224x224 (data augmentation)\n",
    "        transforms.RandomHorizontalFlip(),      # Random horizontal flip (50% probability)\n",
    "        transforms.ToTensor(),                  # Convert PIL Image to tensor [0, 1]\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),                 # Resize shorter side to 256\n",
    "        transforms.CenterCrop(224),             # Center crop to 224x224\n",
    "        transforms.ToTensor(),                  # Convert to tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet stats\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load dataset from directory structure: data/hymenoptera_data/{train,val}/{ants,bees}\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Create data loaders for batching and shuffling\n",
    "# batch_size=4: Process 4 images at a time\n",
    "# shuffle=True: Randomize order each epoch (train only)\n",
    "# num_workers=4: Use 4 subprocesses for data loading (speeds up I/O)\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "# Store dataset sizes for metrics calculation\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "# Get class names automatically from folder names\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Automatic device selection: Use GPU/MPS if available, otherwise CPU\n",
    "# Supports CUDA (NVIDIA), MPS (Apple Silicon), MTIA, XPU\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN3K04PcJKnn"
   },
   "source": [
    "Visualize a few images\n",
    "======================\n",
    "\n",
    "Let\\'s visualize a few training images so as to understand the data\n",
    "augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWaJSHfzJKnn"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"\n",
    "    Display image from tensor.\n",
    "    Reverses normalization to show original image colors.\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy and transpose from (C, H, W) to (H, W, C)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Reverse normalization using ImageNet mean and std\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean  # De-normalize\n",
    "    \n",
    "    # Clip values to valid range [0, 1]\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    # Display image\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data to visualize\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch (arranges multiple images in a grid)\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# Show the grid with class labels\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM4RhKHZJKno"
   },
   "source": [
    "Training the model\n",
    "==================\n",
    "\n",
    "Now, let\\'s write a general function to train a model. Here, we will\n",
    "illustrate:\n",
    "\n",
    "-   Scheduling the learning rate\n",
    "-   Saving the best model\n",
    "\n",
    "In the following, parameter `scheduler` is an LR scheduler object from\n",
    "`torch.optim.lr_scheduler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYSWArZMJKno"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, writer=None):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with validation and checkpointing.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss)\n",
    "        optimizer: Optimization algorithm (e.g., SGD, Adam)\n",
    "        scheduler: Learning rate scheduler\n",
    "        num_epochs: Number of training epochs\n",
    "        writer: TensorBoard SummaryWriter (optional)\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model with best weights loaded\n",
    "        history: Dictionary containing training/validation loss and accuracy per epoch\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    # Track epoch-wise metrics for plotting later\n",
    "    history = {\n",
    "        'train': {'loss': [], 'acc': []},\n",
    "        'val':   {'loss': [], 'acc': []}\n",
    "    }\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        # Save initial model state\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0  # Track best validation accuracy\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode (enables dropout, batchnorm training)\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode (disables dropout, batchnorm eval)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data batches\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    # Move data to device (GPU/CPU)\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Zero the parameter gradients (clear from previous batch)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    # Only track gradients during training phase\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)  # Get predicted class\n",
    "                        loss = criterion(outputs, labels)  # Calculate loss\n",
    "\n",
    "                        # Backward pass + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()      # Compute gradients\n",
    "                            optimizer.step()     # Update weights\n",
    "\n",
    "                    # Accumulate statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)  # Total loss\n",
    "                    running_corrects += torch.sum(preds == labels.data).item()  # Correct predictions\n",
    "                \n",
    "                # Step learning rate scheduler after training phase\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # Calculate epoch metrics\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "                # Save history for plotting\n",
    "                history[phase]['loss'].append(epoch_loss)\n",
    "                history[phase]['acc'].append(epoch_acc)\n",
    "\n",
    "                # Log to TensorBoard if available\n",
    "                if writer is not None:\n",
    "                    writer.add_scalar(f'Loss/{phase}', epoch_loss, epoch)\n",
    "                    writer.add_scalar(f'Accuracy/{phase}', epoch_acc, epoch)\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # Save model if it has best validation accuracy so far\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        # Training complete\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # Load best model weights (from epoch with best validation accuracy)\n",
    "        model.load_state_dict(torch.load(best_model_params_path, weights_only=True))\n",
    "\n",
    "    # Ensure TensorBoard events are written to disk\n",
    "    if writer is not None:\n",
    "        writer.flush()\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy curves.\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary with 'train' and 'val' keys, each containing 'loss' and 'acc' lists\n",
    "        title_prefix: Prefix for plot titles (e.g., \"Finetune: \")\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train']['loss']) + 1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    \n",
    "    # Loss subplot\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, history['train']['loss'], 'b-', label='train loss')\n",
    "    plt.plot(epochs, history['val']['loss'], 'r-', label='val loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{title_prefix}Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy subplot\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, history['train']['acc'], 'b-', label='train acc')\n",
    "    plt.plot(epochs, history['val']['acc'], 'r-', label='val acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title_prefix}Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics: Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, phase='val', class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with detailed metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        phase: 'train' or 'val' - which dataset to evaluate on\n",
    "        class_names: List of class names for display\n",
    "    \n",
    "    Returns:\n",
    "        y_true: Ground truth labels\n",
    "        y_pred: Predicted labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating on {phase} set\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {phase.capitalize()} Set')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nClassification Report - {phase.capitalize()} Set:\")\n",
    "    print(\"-\" * 50)\n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                   target_names=class_names, \n",
    "                                   digits=4)\n",
    "    print(report)\n",
    "    \n",
    "    # Overall Accuracy\n",
    "    accuracy = (np.array(y_true) == np.array(y_pred)).sum() / len(y_true)\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKUnvlsHJKno"
   },
   "source": [
    "Visualizing the model predictions\n",
    "=================================\n",
    "\n",
    "Generic function to display predictions for a few images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqfiZQchJKno"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        num_images: Number of images to display (default: 6)\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Display images with predictions\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)  # Restore original mode\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0odJrNzpJKno"
   },
   "source": [
    "1 Finetuning the ConvNet\n",
    "======================\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRjHkOZ-JKno"
   },
   "outputs": [],
   "source": [
    "# Strategy 1: Full Finetuning - Train all layers\n",
    "# Load pretrained ResNet18 with ImageNet weights\n",
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Get the number of input features for the final fc layer\n",
    "num_ftrs = model_ft.fc.in_features  # 512 for ResNet18\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "# Original fc: nn.Linear(512, 1000) for ImageNet's 1000 classes\n",
    "# New fc: nn.Linear(512, 2) for our 2 classes (ants and bees)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move model to device (GPU/CPU)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Define loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# This means we'll update weights in all layers during backpropagation\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler: Multiply LR by gamma=0.1 every step_size=7 epochs\n",
    "# Epoch 0-6: lr=0.001, Epoch 7-13: lr=0.0001, Epoch 14-20: lr=0.00001, etc.\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD1fHSCDJKno"
   },
   "source": [
    "Train and evaluate\n",
    "==================\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less\n",
    "than a minute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqTck_mGJKnp"
   },
   "outputs": [],
   "source": [
    "# Train the full finetuning model\n",
    "# All layers will be updated during training\n",
    "epoch = 30\n",
    "model_ft, history_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=epoch, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for finetuned model\n",
    "plot_history(history_ft, title_prefix=\"Finetune: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the finetuned model with confusion matrix and classification report\n",
    "y_true_ft, y_pred_ft = evaluate_model(model_ft, phase='val', class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKlV63OzJKnp"
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 ConvNet as fixed feature extractor\n",
    "==================================\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set `requires_grad = False` to freeze the parameters so that the\n",
    "gradients are not computed in `backward()`.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "[here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Feature Extractor - Freeze all layers except fc\n",
    "# Load pretrained ResNet18 model\n",
    "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze all convolutional layers (no gradient computation = no weight updates)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fc layer\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "# So only this layer will be trained\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move to device\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "# model_conv.fc.parameters() returns only the fc layer parameters\n",
    "# All other layers are frozen and won't be updated\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "==================\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don\\'t need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the feature extractor model (only fc layer is trainable)\n",
    "epoch = 30\n",
    "model_conv, history_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=epoch, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for feature extractor model\n",
    "plot_history(history_conv, title_prefix=\"Feature extractor: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the feature extractor model with confusion matrix and classification report\n",
    "y_true_conv, y_pred_conv = evaluate_model(model_conv, phase='val', class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on custom images\n",
    "==========================\n",
    "\n",
    "Use the trained model to make predictions on custom images and visualize\n",
    "the predicted class labels along with the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(model, img_path):\n",
    "    \"\"\"\n",
    "    Make predictions on a single custom image and display result.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        img_path: Path to image file\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(img_path)\n",
    "    img = data_transforms['val'](img)  # Apply validation transforms\n",
    "    img = img.unsqueeze(0)  # Add batch dimension: (C, H, W) -> (1, C, H, W)\n",
    "    img = img.to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Display image with prediction\n",
    "        ax = plt.subplot(2,2,1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
    "        imshow(img.cpu().data[0])\n",
    "\n",
    "        model.train(mode=was_training)  # Restore original mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbckCC4HJKnp"
   },
   "source": [
    "3 Finetuning the ConvNet - last 2 CNN layers + fc\n",
    "======================\n",
    "\n",
    "Load a pretrained model and unfreeze the last 2 convolutional blocks (layer4) plus the final fc layer.\n",
    "- ResNet18 architecture: conv1 → layer1 → layer2 → layer3 → layer4 → fc\n",
    "- We freeze: conv1, layer1, layer2, layer3\n",
    "- We train: layer4 (last 2 residual blocks) + fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaNUBTAcJKnp"
   },
   "outputs": [],
   "source": [
    "# Strategy 3: Partial Finetuning - Train last 2 CNN blocks (layer4) + fc\n",
    "# This is a middle ground between full finetuning and feature extraction\n",
    "# Load pretrained ResNet18 model\n",
    "model_partial = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Freeze all parameters first\n",
    "for param in model_partial.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze layer4 (last 2 CNN blocks) - these are the last convolutional layers before fc\n",
    "# ResNet18 layer4 contains 2 BasicBlocks, each with 2 conv layers = 4 conv layers total\n",
    "for param in model_partial.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace and unfreeze the final fc layer (this is automatic since it's newly created)\n",
    "num_ftrs = model_partial.fc.in_features\n",
    "model_partial.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Move to device\n",
    "model_partial = model_partial.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize layer4 + fc with potentially different learning rates\n",
    "# Use a smaller lr for layer4 (pretrained features) and normal lr for fc (random init)\n",
    "# This is a common strategy: fine-tune pretrained layers slowly, train new layers faster\n",
    "optimizer_partial = optim.SGD([\n",
    "    {'params': model_partial.layer4.parameters(), 'lr': 0.0001},  # Lower LR for fine-tuning CNN\n",
    "    {'params': model_partial.fc.parameters(), 'lr': 0.001}        # Higher LR for new fc\n",
    "], momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler (applies to all parameter groups)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_partial, step_size=7, gamma=0.1)\n",
    "\n",
    "# Print trainable parameter counts for verification\n",
    "print(\"Trainable parameters:\")\n",
    "print(\"- layer4 (last 2 CNN blocks):\", sum(p.numel() for p in model_partial.layer4.parameters() if p.requires_grad))\n",
    "print(\"- fc (final classifier):\", sum(p.numel() for p in model_partial.fc.parameters() if p.requires_grad))\n",
    "print(\"- Total trainable:\", sum(p.numel() for p in model_partial.parameters() if p.requires_grad))\n",
    "print(\"- Total parameters:\", sum(p.numel() for p in model_partial.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "model_partial, history_partial = train_model(model_partial, criterion, optimizer_partial,\n",
    "                         exp_lr_scheduler, num_epochs=epoch, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for partial fine-tuning (layer4 + fc)\n",
    "plot_history(history_partial, title_prefix=\"Partial finetune (layer4+fc): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the partial finetuned model (layer4 + fc) with confusion matrix and classification report\n",
    "y_true_partial, y_pred_partial = evaluate_model(model_partial, phase='val', class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_partial)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcX6MDTSJKnq"
   },
   "source": [
    "Inference on custom images\n",
    "==========================\n",
    "\n",
    "Use the trained model to make predictions on custom images and visualize\n",
    "the predicted class labels along with the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJq1LlkKJKnq"
   },
   "outputs": [],
   "source": [
    "visualize_model_predictions(\n",
    "    model_partial,\n",
    "    img_path='data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGdU5X_MJKnq"
   },
   "source": [
    "Further Learning\n",
    "================\n",
    "\n",
    "If you would like to learn more about the applications of transfer\n",
    "learning, checkout our [Quantized Transfer Learning for Computer Vision\n",
    "Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close TensorBoard writer (optional but good practice)\n",
    "# This ensures all logged events are written to disk\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "    print(\"TensorBoard writer closed.\")\n",
    "    print(\"\\nTo view results, run: tensorboard --logdir runs/transfer_learning\")\n",
    "    print(\"Then open http://localhost:6006 in your browser\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
