{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model\n",
    "- tensorflow hub: https://www.tensorflow.org/hub\n",
    "- pytorch hub: https://pytorch.org/hub/\n",
    "- 노트북에서 실행시간 매우 길어 실습 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H1NGz91sqwZW"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch  # PyTorch 텐서 및 연산\n",
    "import torch.nn as nn  # 신경망 레이어/모듈 (Linear, Conv 등) 및 손실 함수\n",
    "import torch.optim as optim  # 최적화 알고리즘 (SGD, Adam 등)\n",
    "from torch.utils.data import DataLoader, TensorDataset  # 데이터 로딩 유틸 및 텐서 기반 데이터셋\n",
    "import numpy as np  # 수치 연산, 난수 생성 등에 사용\n",
    "from torchvision import models, transforms  # 사전학습(Pretrained) 모델과 이미지 전처리 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZKS2yJbrq65t"
   },
   "outputs": [],
   "source": [
    "# 데이터 설정 및 더미 데이터 생성\n",
    "num_classes = 10  # 분류할 클래스 개수\n",
    "input_shape = (3, 224, 224)  # EfficientNetB0의 기본 입력 크기 (채널=3, 224x224)\n",
    "\n",
    "# 데모를 위한 난수 입력/레이블 생성 (실전에서는 실제 이미지와 라벨을 사용)\n",
    "x_train = np.random.random((1000, 3, 224, 224)).astype(np.float32)  # 1,000개 학습용 이미지\n",
    "y_train = np.random.randint(num_classes, size=(1000,))  # 0~9 사이의 정수 라벨\n",
    "x_test = np.random.random((200, 3, 224, 224)).astype(np.float32)   # 200개 테스트용 이미지\n",
    "y_test = np.random.randint(num_classes, size=(200,))  # 테스트 라벨\n",
    "\n",
    "# 텐서로 변환하여 TensorDataset 구성 (이미지가 이미 텐서 형태이므로 추가 변환 불필요)\n",
    "train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
    "\n",
    "# DataLoader로 배치 단위 로딩 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)   # 학습 데이터는 섞음\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)    # 테스트 데이터는 순서 유지\n",
    "\n",
    "# 참고: 실제 이미지 데이터를 사용할 때는 transforms.Resize(224), transforms.Normalize 등 전처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSr3uZI5q8xr",
    "outputId": "f0581934-e3d1-4ea2-f7a3-86452fa30c3b"
   },
   "outputs": [],
   "source": [
    "# 사전훈련 모델 불러오기 및 출력층 수정\n",
    "# EfficientNetB0 (ImageNet으로 사전훈련된 가중치 사용)\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# 분류기(마지막 Linear 레이어)를 현재 데이터셋의 클래스 수에 맞게 교체 (1000 → num_classes)\n",
    "# EfficientNetB0의 classifier는 [Dropout, Linear] 구조이므로 index 1의 Linear를 교체\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# 참고:\n",
    "# - 전이학습에서 feature extractor를 고정하고 싶다면, 다음처럼 requires_grad를 끌 수 있음\n",
    "#   for param in model.features.parameters():\n",
    "#       param.requires_grad = False\n",
    "# - 입력 텐서는 (N, 3, 224, 224) 형식이어야 하며, ImageNet 정규화로 학습된 모델이므로\n",
    "#   실제 이미지에서는 transforms.Normalize(mean, std)를 적용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m9dlcgoGrBDw"
   },
   "outputs": [],
   "source": [
    "# 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류에 일반적으로 사용하는 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저 (작은 학습률 권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrTGbyryrEEH",
    "outputId": "b65a3662-7165-4442-b5de-48408e7fe1bd"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 루프 (간단 버전) \n",
    "# 노트북에서 실행시간 매우 길어 실습 어려움\n",
    "\n",
    "num_epochs = 3  # 사전훈련 모델은 빠르게 수렴하는 경향이 있어 데모에서는 짧게 설정\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 학습 모드로 전환 (Dropout/BatchNorm 활성화 동작)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # (옵션) GPU 사용 시: inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()      # 이전 배치의 기울기 초기화\n",
    "        outputs = model(inputs)    # 순전파(Forward): 예측값 계산\n",
    "        loss = criterion(outputs, labels)  # 손실 계산\n",
    "        loss.backward()            # 역전파(Backward): 기울기 계산\n",
    "        optimizer.step()           # 가중치 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqfR5X56rF_4",
    "outputId": "7c93afc4-8575-4943-be2b-9fca9225518b"
   },
   "outputs": [],
   "source": [
    "# 모델 평가 (정확도 측정)\n",
    "model.eval()  # 평가 모드로 전환 (Dropout/BatchNorm 비활성화 동작)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # 평가 시 기울기 계산 비활성화 (메모리/속도 이점)\n",
    "    for inputs, labels in test_loader:\n",
    "        # (옵션) GPU 사용 시: inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)             # 예측값 계산\n",
    "        _, predicted = torch.max(outputs, 1)  # 각 샘플의 최대 점수 클래스 선택\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
