{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97673829",
   "metadata": {
    "id": "97673829"
   },
   "source": [
    "# ResNet\n",
    "\n",
    "*Author: Pytorch Team*\n",
    "\n",
    "**Deep residual networks pre-trained on ImageNet**\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/resnet.png\" alt=\"alt\" width=\"50%\"/>\n",
    "\n",
    "- https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "- https://arxiv.org/pdf/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current library versions\n",
    "import torch, torchvision\n",
    "print('torch:', torch.__version__)\n",
    "print('torchvision:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f121702",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f121702",
    "outputId": "1bc553aa-4326-4a23-ac67-98b203e42451"
   },
   "outputs": [],
   "source": [
    "# Load pretrained ResNet18 via torch.hub\n",
    "# Note: torch.hub can raise ImportError if your PyTorch/torchvision versions are incompatible.\n",
    "# - Using weights=True is deprecated in newer torchvision; the recommended way is to use the Weights Enum\n",
    "#   with torchvision.models (shown below as a commented alternative).\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=True)\n",
    "\n",
    "# Examples for other ResNet variants (same hub snapshot):\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode (BatchNorm/Dropout behave accordingly)\n",
    "model.eval()\n",
    "\n",
    "# Recommended alternative (commented): avoid torch.hub and use torchvision.models with weight enums\n",
    "# from torchvision import models\n",
    "# from torchvision.models import ResNet18_Weights\n",
    "# model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398ac34",
   "metadata": {
    "id": "4398ac34"
   },
   "source": [
    "All pre-trained models expect input images normalized in the same way,\n",
    "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
    "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
    "and `std = [0.229, 0.224, 0.225]`.\n",
    "\n",
    "Here's a sample execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4ba4d",
   "metadata": {
    "id": "fea4ba4d"
   },
   "outputs": [],
   "source": [
    "# Download an example image (PyTorch Hub sample)\n",
    "# - Requires network access. It may fail behind firewalls or proxies.\n",
    "import urllib\n",
    "\n",
    "# GitHub sample image URL and local filename\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "\n",
    "# Use a try/except to handle differences across Python versions\n",
    "try:\n",
    "    # Legacy style (works in some environments)\n",
    "    urllib.URLopener().retrieve(url, filename)\n",
    "except Exception:\n",
    "    # Standard Python 3 style\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# After saving, we load and display it in the next cell with PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82496c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the downloaded image\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open the saved image file (RGB)\n",
    "input_image = PIL.Image.open(filename)\n",
    "\n",
    "# Display with matplotlib for a quick sanity check\n",
    "plt.imshow(input_image)\n",
    "# plt.axis('off')  # Uncomment to hide axes\n",
    "plt.title('Downloaded example image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b01e0",
   "metadata": {},
   "source": [
    "## Why normalize with ImageNet statistics?\n",
    "\n",
    "Pretrained models (e.g., ResNet) were trained with input images normalized per-channel by ImageNet mean and std. Applying the same normalization at inference time aligns the input distribution with training.\n",
    "\n",
    "- Match training distribution: Use the same mean/std to avoid performance drops from distribution shift.\n",
    "- Correct channel bias: R/G/B channels have different average brightness/variance; (x - mean) / std balances scale per channel.\n",
    "- Numerical stability: Prevents saturation or vanishing/exploding effects due to extreme input scales.\n",
    "- Reproducibility/generalization: Stabilizes feature ranges so the model behaves robustly across images.\n",
    "\n",
    "ImageNet recommended stats:\n",
    "- mean = [0.485, 0.456, 0.406]\n",
    "- std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "Code (same as in this notebook):\n",
    "```python\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "```\n",
    "\n",
    "Notes\n",
    "- When reusing ImageNet-pretrained weights, use the ImageNet stats above.\n",
    "- If training from scratch on your own dataset, consider computing and using your dataset's mean/std."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af916d-9e9f-4839-8503-eaac6c3b87cd",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3267db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc3267db",
    "outputId": "3ede20d8-5919-4b93-f5e4-4b79c5373e3c"
   },
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline and run inference\n",
    "# - ImageNet pretrained models expect RGB images with shape (3, 224, 224).\n",
    "# - Process input images with Resize → CenterCrop → ToTensor → Normalize.\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Open image from file (RGB)\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),                      # Resize the shorter side to 256\n",
    "    transforms.CenterCrop(224),                  # Center crop to 224x224\n",
    "    transforms.ToTensor(),                       # PIL [0..255] → float tensor [0..1] (C, H, W)\n",
    "    transforms.Normalize(                        # Normalize with ImageNet channel-wise mean/std\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Create (3, 224, 224) tensor\n",
    "input_tensor = preprocess(input_image)\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# Add batch dimension → (1, 3, 224, 224)\n",
    "input_batch = input_tensor.unsqueeze(0)  # The model expects batched inputs\n",
    "print(input_batch.shape)\n",
    "\n",
    "# Move to GPU if available (for speed)\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3b705-29c8-4f08-a2dc-43facfe63cf5",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44aae9-e013-4604-afac-853ba164f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (disable grad for speed/memory)\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Output tensor: (1, 1000) — logits for ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "print(output[0].shape)\n",
    "\n",
    "# Convert logits to probabilities (sum to 1). dim=0 applies over the class dimension\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83644362",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83644362",
    "outputId": "c5c2d455-aade-45bd-a89c-127a80fdd0f0"
   },
   "outputs": [],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a6457",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd2a6457",
    "outputId": "043d413b-776a-433b-b061-7fa7afc3a14e"
   },
   "outputs": [],
   "source": [
    "# Load ImageNet class labels and print Top-5 results\n",
    "# - Assumes imagenet_classes.txt was downloaded via wget in the previous cell.\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]  # strip newline per line\n",
    "\n",
    "# Get top-5 probabilities and their class indices from the probability vector\n",
    "# torch.topk returns (values, indices); we request k=5\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "print('Top-5 predictions:')\n",
    "for i in range(top5_prob.size(0)):\n",
    "    class_name = categories[top5_catid[i]]\n",
    "    prob_value = top5_prob[i].item()\n",
    "    print(f\"{i+1}. {class_name:25s}  prob={prob_value:.4f}\")\n",
    "\n",
    "# Note: Alternatively, you can take top-5 from logits via torch.topk(output[0], 5)\n",
    "# and then apply softmax to compute probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc745ef3",
   "metadata": {},
   "source": [
    "## Top-N error\n",
    "\n",
    "- Top-1 error\n",
    "  - Definition: The fraction of images where the model’s single most-confident prediction (argmax) is NOT the ground-truth label.\n",
    "  - Relation to accuracy: top-1 error = 1 − top-1 accuracy.\n",
    "  - Example: If top-1 error is 30.24%, then top-1 accuracy is 69.76%.\n",
    "\n",
    "- Top-5 error\n",
    "  - Definition: The fraction of images where the ground-truth label is NOT among the model’s five most-confident predictions.\n",
    "  - Relation to accuracy: top-5 error = 1 − top-5 accuracy.\n",
    "  - Intuition: More forgiving for fine-grained classes (e.g., different dog breeds). If the true class appears anywhere in the top 5 guesses, it counts as correct for top-5.\n",
    "\n",
    "- Lower is better for both. A top-5 error of 10.92% means in 89.08% of images, the correct class is within the model’s top 5 predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee99c77-a822-4748-9594-d4fb1a5c41fd",
   "metadata": {},
   "source": [
    "```python\n",
    "# logits: torch.Tensor of shape (N, 1000)\n",
    "# target: torch.Tensor of shape (N,) with class indices\n",
    "\n",
    "top1_correct = (logits.argmax(dim=1) == target).float().mean()\n",
    "top5_correct = logits.topk(5, dim=1).indices.eq(target.unsqueeze(1)).any(dim=1).float().mean()\n",
    "\n",
    "top1_error = 1.0 - top1_correct.item()\n",
    "top5_error = 1.0 - top5_correct.item()\n",
    "print(f\"top-1 error: {top1_error:.4f}, top-5 error: {top5_error:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8122b",
   "metadata": {},
   "source": [
    "## Top-5 accuracy vs. \"1 − sum of top‑5 probabilities\"\n",
    "\n",
    "Top‑5 accuracy is a 0/1 correctness check per sample: it is 1 if the ground‑truth class index is among the top‑5 predicted classes (by score), else 0. The dataset/batch top‑5 accuracy is the average of those 0/1 values.\n",
    "\n",
    "It is NOT computed as 1 minus the sum of the top‑5 probabilities. The sum of top‑5 probabilities can be large even when the true label is not in the top‑5 (accuracy=0), and it can be smaller even when the true label is inside the top‑5 (accuracy=1).\n",
    "\n",
    "Two toy examples:\n",
    "- Example A (wrongly classified but top‑5 probs sum is large):\n",
    "  - probs = [0.20, 0.19, 0.18, 0.17, 0.16, 0.10], true label = index 5\n",
    "  - top‑5 indices = [0,1,2,3,4] → ground truth NOT in top‑5 → top‑5 accuracy = 0\n",
    "  - sum(top‑5 probs) = 0.90 → 1 − 0.90 = 0.10 (this is NOT the top‑5 error)\n",
    "- Example B (correctly classified but top‑5 probs sum is smaller):\n",
    "  - probs = [0.21, 0.20, 0.19, 0.18, 0.17, 0.05], true label = index 4\n",
    "  - top‑5 indices = [0,1,2,3,4] → ground truth IS in top‑5 → top‑5 accuracy = 1\n",
    "  - sum(top‑5 probs) = 0.95 → 1 − 0.95 = 0.05 (still NOT the top‑5 error)\n",
    "\n",
    "In short: top‑5 accuracy checks membership of the true class in the top‑5 set (rank-based), not the magnitude of probabilities. Softmax is only needed if you want probabilities for display; the ranking (top‑k) is identical on logits and on softmaxed probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24f867",
   "metadata": {
    "id": "de24f867"
   },
   "source": [
    "### Model Description\n",
    "\n",
    "Resnet models were proposed in \"Deep Residual Learning for Image Recognition\".\n",
    "Here we have the 5 versions of resnet models, which contains 18, 34, 50, 101, 152 layers respectively.\n",
    "Detailed model architectures can be found in Table 1.\n",
    "Their 1-crop error rates on ImageNet dataset with pretrained models are listed below.\n",
    "\n",
    "| Model structure | Top-1 error | Top-5 error |\n",
    "| --------------- | ----------- | ----------- |\n",
    "|  resnet18       | 30.24       | 10.92       |\n",
    "|  resnet34       | 26.70       | 8.58        |\n",
    "|  resnet50       | 23.85       | 7.13        |\n",
    "|  resnet101      | 22.63       | 6.44        |\n",
    "|  resnet152      | 21.69       | 5.94        |\n",
    "\n",
    "### References\n",
    "\n",
    " - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
