{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ae35c7",
   "metadata": {},
   "source": [
    "1. 자신이 생각하는 가장 깔끔한 트랜스포머 모델을 파이토치로 구현하고 리얼한 데이터셋으로 학습과 추론을 해보세요. \n",
    "\n",
    "2. 1번의 코드를 바탕으로 트랜스포머의 얼개를 설명하는 보고서를 작성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657]])\n",
      "Output length: 14\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # d_in= 임베딩 차원\n",
    "    # d_out= 임베딩 차원\n",
    "    # context_length= 한번에 처리 가능한 문맥크기\n",
    "    # num_heads= 어텐션 헤드 개수\n",
    "    # dropout= 드롭아웃 비율\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 출력차원을 어텐션헤드의 개수로 나눈다.        \n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        # Linear도 결국 (d_in, d_out)크기의 행렬\n",
    "        # 가중치 배열을 초기화하는 부분\n",
    "        # (임베딩 크기, 임베딩 크기)만큼의 행렬을 생성한다.\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # 어텐션 출력을 병합하기 위한 레이어\n",
    "        self.out_proj = nn.Linear(d_out, d_out) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 마스킹을 위한 행렬\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    # 입력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 가중치배열과 연산하여 Q, K, V를 만드는 부분\n",
    "        # (6, 3) * (3, 3) = (6, 3)\n",
    "        keys = self.W_key(x)  # Shape: (배치크기, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 차원을 추가하여 head_dim 행렬을 나눈다.\n",
    "        # 마지막 차원을 나눈다: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        # 1번, 2번 인덱스 차원의 행과 열을 바꾼다.\n",
    "        # 각 헤드의 토큰을 병렬적으로 처리하기 위해 바꾸는 과정\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # queries : (b, num_heads, num_tokens, head_dim)\n",
    "        # keys.transpose(2,3) : (b, num_heads, head_dim, num_tokens)\n",
    "        # 출력 : (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # 마스킹 연산 수행\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\n",
    "        # attn_weights : (b, num_heads, num_tokens, num_tokens)\n",
    "        # values : (b, num_heads, num_tokens, head_dim)\n",
    "        # 출력: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # contiguous() : view()함수를 사용하기 위해 벡터들을 메모리상에서 순서대로 배치시킴\n",
    "        # view() : 분리된 헤드를 결합시켜 (b, num_tokens, d_out)와 같은 입력벡터와 동일한 차원으로 만든다.\n",
    "        # 헤드 결합, self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # 어텐션 블록이 여러 헤드를 통해 얻은 문맥 정보를 최종적으로 요약하고 압축하여 모델의 다음 단계로 전달하는 역할\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    # 정규화하여 입력과 같은 차원의 출력을 반환\n",
    "    # scale, shift파라미터를 조정하여 성능향상\n",
    "    def forward(self, x):\n",
    "        # 소프트맥스 정규화 : 평균이 0, 분산이 1이 되도록 계산\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 토큰 임베딩의 차원을 입력으로 받아서 4배로 차원을 확장시키고\n",
    "        # GELU함수 적용\n",
    "        # 4배 확장된 입력을 받아서 원래 차원으로 다시 복귀시킴\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    # 입력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "    def forward(self, x):\n",
    "        ##### 멀티헤드어텐션 연산 부분\n",
    "        # 잔차연결값 저장\n",
    "        shortcut = x\n",
    "        # 정규화\n",
    "        x = self.norm1(x)\n",
    "        # 멀티헤드어텐션 연산 수행\n",
    "        x = self.att(x)   # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        # 드롭아웃\n",
    "        x = self.drop_shortcut(x)\n",
    "        # 잔차연결\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        ##### 피드포워드 연산 부분\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x) # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 각 단어들을 임베딩 토큰으로 변경하기 위한 조회 테이블\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Sequential: 배열에 담긴 레이어를 묶어서 한번에 관리하기 위함\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # 모델입력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수, 현재배치의 토큰 수)\n",
    "        # 예를들어 (2:배치크기, 4: 토큰 수)이 입력인 경우 (2, 4, 임베딩 크기)로 벡터를 변환시킴\n",
    "        # 토큰 임베딩 이후 포지셔널 임베딩 벡터와 합산\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # 멀티헤드어텐션 연산 수행\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # 마지막 정규화층 연산\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # out_head = Linear(단어임베딩 크기, 단어사전 크기)\n",
    "        # 각각의 단어를 선형층을 통해 단어 마다의 확률로 변환시키는 과정\n",
    "        # out_head의 선형층을 거치면서 마지막 차원의 크기가 (단어임베딩 크기 -> 단어사전 크기) 로 확장된다.\n",
    "        # 모델출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 각 토큰마다 단어사전의 확률분포)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# idx : 초기 입력\n",
    "# max_new_tokens : 생성할 최대 토큰\n",
    "# context_size : 모델이 한 번에 볼 수 있는 문맥크기\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # max_new_tokens개 만큼의 토큰이 생성 될 때까지 반복\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # 모델이 한번에 처리 가능한 문맥 크기를 넘는 경우, 앞부분을 자르기 위한 코드\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 모델입력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수, 현재배치의 토큰 수)\n",
    "            # 모델출력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수), 현재배치의 토큰 수, 각 토큰마다 단어사전의 확률분포)\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 마지막 단어의 확률분포만 남김\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어만 남김\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어를 기존 문맥에 이어붙임\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def main():\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    start_context = \"Hello, I am\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "\n",
    "    # 차원의 0번 인덱스의 새로운 차원을 추가 ( ,4) -> (1, 4)\n",
    "    # 모델은 배치로 실행되기 때문에 배치 차원을 추가하는 과정이다.\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "    out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2차원으로 나온 결과의 0번 인덱스 차원을 제거하여 1차원으로 변환\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3e540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # d_in= 임베딩 차원\n",
    "    # d_out= 임베딩 차원\n",
    "    # context_length= 한번에 처리 가능한 문맥크기\n",
    "    # num_heads= 어텐션 헤드 개수\n",
    "    # dropout= 드롭아웃 비율\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 출력차원을 어텐션헤드의 개수로 나눈다.        \n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        # Linear도 결국 (d_in, d_out)크기의 행렬\n",
    "        # 가중치 배열을 초기화하는 부분\n",
    "        # (임베딩 크기, 임베딩 크기)만큼의 행렬을 생성한다.\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # 어텐션 출력을 병합하기 위한 레이어\n",
    "        self.out_proj = nn.Linear(d_out, d_out) \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 마스킹을 위한 행렬\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    # 입력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 가중치배열과 연산하여 Q, K, V를 만드는 부분\n",
    "        # (6, 3) * (3, 3) = (6, 3)\n",
    "        keys = self.W_key(x)  # Shape: (배치크기, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 차원을 추가하여 head_dim 행렬을 나눈다.\n",
    "        # 마지막 차원을 나눈다: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        # 1번, 2번 인덱스 차원의 행과 열을 바꾼다.\n",
    "        # 각 헤드의 토큰을 병렬적으로 처리하기 위해 바꾸는 과정\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # queries : (b, num_heads, num_tokens, head_dim)\n",
    "        # keys.transpose(2,3) : (b, num_heads, head_dim, num_tokens)\n",
    "        # 출력 : (b, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # 마스킹 연산 수행\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "\n",
    "        # attn_weights : (b, num_heads, num_tokens, num_tokens)\n",
    "        # values : (b, num_heads, num_tokens, head_dim)\n",
    "        # 출력: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # contiguous() : view()함수를 사용하기 위해 벡터들을 메모리상에서 순서대로 배치시킴\n",
    "        # view() : 분리된 헤드를 결합시켜 (b, num_tokens, d_out)와 같은 입력벡터와 동일한 차원으로 만든다.\n",
    "        # 헤드 결합, self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        # 어텐션 블록이 여러 헤드를 통해 얻은 문맥 정보를 최종적으로 요약하고 압축하여 모델의 다음 단계로 전달하는 역할\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    # 정규화하여 입력과 같은 차원의 출력을 반환\n",
    "    # scale, shift파라미터를 조정하여 성능향상\n",
    "    def forward(self, x):\n",
    "        # 소프트맥스 정규화 : 평균이 0, 분산이 1이 되도록 계산\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 토큰 임베딩의 차원을 입력으로 받아서 4배로 차원을 확장시키고\n",
    "        # GELU함수 적용\n",
    "        # 4배 확장된 입력을 받아서 원래 차원으로 다시 복귀시킴\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    # 입력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "    def forward(self, x):\n",
    "        ##### 멀티헤드어텐션 연산 부분\n",
    "        # 잔차연결값 저장\n",
    "        shortcut = x\n",
    "        # 정규화\n",
    "        x = self.norm1(x)\n",
    "        # 멀티헤드어텐션 연산 수행\n",
    "        x = self.att(x)   # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        # 드롭아웃\n",
    "        x = self.drop_shortcut(x)\n",
    "        # 잔차연결\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        ##### 피드포워드 연산 부분\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x) # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 각 단어들을 임베딩 토큰으로 변경하기 위한 조회 테이블\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Sequential: 배열에 담긴 레이어를 묶어서 한번에 관리하기 위함\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # 모델입력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수, 현재배치의 토큰 수)\n",
    "        # 예를들어 (2:배치크기, 4: 토큰 수)이 입력인 경우 (2, 4, 임베딩 크기)로 벡터를 변환시킴\n",
    "        # 토큰 임베딩 이후 포지셔널 임베딩 벡터와 합산\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # 멀티헤드어텐션 연산 수행\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # 마지막 정규화층 연산\n",
    "        # 출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 토큰 임베딩 크기)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # out_head = Linear(단어임베딩 크기, 단어사전 크기)\n",
    "        # 각각의 단어를 선형층을 통해 단어 마다의 확률로 변환시키는 과정\n",
    "        # out_head의 선형층을 거치면서 마지막 차원의 크기가 (단어임베딩 크기 -> 단어사전 크기) 로 확장된다.\n",
    "        # 모델출력 : (배치크기 :한번에 병렬적으로 추론할 단어의 수, 현재배치의 토큰 수, 각 토큰마다 단어사전의 확률분포)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# idx : 초기 입력\n",
    "# max_new_tokens : 생성할 최대 토큰\n",
    "# context_size : 모델이 한 번에 볼 수 있는 문맥크기\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # max_new_tokens개 만큼의 토큰이 생성 될 때까지 반복\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # 모델이 한번에 처리 가능한 문맥 크기를 넘는 경우, 앞부분을 자르기 위한 코드\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 모델입력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수, 현재배치의 토큰 수)\n",
    "            # 모델출력 : (배치크기 :한번에 병렬적으로 추론할 문장의 수), 현재배치의 토큰 수, 각 토큰마다 단어사전의 확률분포)\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 마지막 단어의 확률분포만 남김\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어만 남김\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어를 기존 문맥에 이어붙임\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def main():\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"emb_dim\": 768,          # Embedding dimension\n",
    "        \"n_heads\": 12,           # Number of attention heads\n",
    "        \"n_layers\": 12,          # Number of layers\n",
    "        \"drop_rate\": 0.1,        # Dropout rate\n",
    "        \"qkv_bias\": False        # Query-Key-Value bias\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.eval()  # disable dropout\n",
    "\n",
    "    start_context = \"Hello, I am\"\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "\n",
    "    # 차원의 0번 인덱스의 새로운 차원을 추가 ( ,4) -> (1, 4)\n",
    "    # 모델은 배치로 실행되기 때문에 배치 차원을 추가하는 과정이다.\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "    out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=10,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2차원으로 나온 결과의 0번 인덱스 차원을 제거하여 1차원으로 변환\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65dcdc08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpt_module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Import from local files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpt_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTModel, create_dataloader_v1, generate_text_simple\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtext_to_token_ids\u001b[39m(text, tokenizer):\n\u001b[32m     13\u001b[39m     encoded = tokenizer.encode(text)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gpt_module'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# Import from local files\n",
    "from gpt_module import GPTModel, create_dataloader_v1, generate_text_simple\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def main(gpt_config, settings):\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ##############################\n",
    "    # Download data if necessary\n",
    "    ##############################\n",
    "\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    ##############################\n",
    "    # Initialize model\n",
    "    ##############################\n",
    "\n",
    "    model = GPTModel(gpt_config)\n",
    "    model.to(device)  # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    ##############################\n",
    "    # Set up dataloaders\n",
    "    ##############################\n",
    "\n",
    "    # Train/validation ratio\n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "    train_loader = create_dataloader_v1(\n",
    "        text_data[:split_idx],\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloader_v1(\n",
    "        text_data[split_idx:],\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    ##############################\n",
    "    # Train model\n",
    "    ##############################\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=settings[\"num_epochs\"], eval_freq=5, eval_iter=1,\n",
    "        start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    return train_losses, val_losses, tokens_seen, model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,    # Vocabulary size\n",
    "        \"context_length\": 256,  # Shortened context length (orig: 1024)\n",
    "        \"emb_dim\": 768,         # Embedding dimension\n",
    "        \"n_heads\": 12,          # Number of attention heads\n",
    "        \"n_layers\": 12,         # Number of layers\n",
    "        \"drop_rate\": 0.1,       # Dropout rate\n",
    "        \"qkv_bias\": False       # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    OTHER_SETTINGS = {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 2,\n",
    "        \"weight_decay\": 0.1\n",
    "    }\n",
    "\n",
    "    ###########################\n",
    "    # Initiate training\n",
    "    ###########################\n",
    "\n",
    "    train_losses, val_losses, tokens_seen, model = main(GPT_CONFIG_124M, OTHER_SETTINGS)\n",
    "\n",
    "    ###########################\n",
    "    # After training\n",
    "    ###########################\n",
    "\n",
    "    # Plot results\n",
    "    epochs_tensor = torch.linspace(0, OTHER_SETTINGS[\"num_epochs\"], len(train_losses))\n",
    "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "    plt.savefig(\"loss.pdf\")\n",
    "\n",
    "    # Save and load model\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdcb5e",
   "metadata": {},
   "source": [
    "3. 주어진 데이터셋 중 원하는 데이터셋을 사용해 원하는 사전 훈련 모델(CNN)을 훈련시키고, 연구보고서를 작성해 보세요.\n",
    "- (필수) Test Accuracy, Test Loss 출력 값과 성능 시각화(그래프)를 포함한 연구 및 분석 과정에 대한 내용이 있어야합니다.\n",
    "- (선택) 위 성공 조건을 달성했다면. 실제 데이터를 입력해 예측 또는 분류 결과 출력 후 분석한 내용도 작성해보세요.\n",
    "\n",
    "\n",
    "아래 항목을 포함하여 모델의 학습 및 성능을 분석하는 내용을 중심으로 작성하세요.\n",
    "- **Train Accuracy / Train Loss / Test Accuracy / Test Loss**\n",
    "- **성능 시각화 (정확도·손실 곡선, 혼동 행렬 등)**\n",
    "- **결과 해석 및 개선 방향 제시**\n",
    "- **모델 설계 및 파라미터 선택 이유**\n",
    "\n",
    "단순히 수치를 나열하지 말고, “왜 이런 결과가 나왔는가”, “무엇을 시도했고, 어떤 교훈을 얻었는가”**를 논리적으로 설명해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# A100 GPU 최적화 설정\n",
    "torch.backends.cudnn.benchmark = True  # 최적의 알고리즘 자동 선택\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # TF32 활성화 (A100 특화)\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "\n",
    "def get_CIFAR10_data_loader(ratio:int = 1.0)-> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    CIFAR10데이터셋을 가져오는 함수 (전체 훈련셋 : 50000개, 전체 테스트셋 : 10000개)\n",
    "\n",
    "    Args:\n",
    "        ratio: 전체 데이터셋 대비 비중\n",
    "\n",
    "    Returns:\n",
    "        (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    TRAINSET_COUNT = 50000\n",
    "\n",
    "    # 1. 전처리 정의 (GPU 친화적)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train\n",
    "    )\n",
    "\n",
    "    # 데이터 서브셋 생성\n",
    "    indices = list(range(int(TRAINSET_COUNT * ratio)))\n",
    "    subset_trainset = Subset(trainset, indices)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    # 3. DataLoader 생성 (A100 최적화)\n",
    "    # A100은 대용량 배치를 효율적으로 처리\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        subset_trainset,\n",
    "        batch_size=64,  # A100에 최적화된 큰 배치 사이즈\n",
    "        shuffle=True,\n",
    "        num_workers=12,  # A100 시스템의 CPU 코어 활용\n",
    "        pin_memory=True,  # GPU 전송 속도 향상\n",
    "        prefetch_factor=4,  # 미리 가져올 배치 수\n",
    "        persistent_workers=True  # Worker 재사용\n",
    "    )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=512,  # 평가 시 더 큰 배치\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "def get_custom_resnet50(class_count:int = 10) -> models.ResNet:\n",
    "    \"\"\"\n",
    "    resnet의 분류클래스를 변경하여 리턴하는 함수\n",
    "\n",
    "    Args:\n",
    "        클래스의 개수\n",
    "\n",
    "    Returns:\n",
    "        ResNet모델\n",
    "    \"\"\"\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    # 모델의 마지막 완전연결층 레이어 변경\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, class_count)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_custom_vgg16(class_count:int = 10) -> models.vgg16:\n",
    "    \"\"\"\n",
    "    VGG16의 분류클래스를 변경하여 리턴하는 함수\n",
    "\n",
    "    Args:\n",
    "        클래스의 개수\n",
    "\n",
    "    Returns:\n",
    "        VGG16모델\n",
    "    \"\"\"\n",
    "    model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    in_features = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(in_features, 10)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model, loss_function, test_loader, device):\n",
    "    \"\"\"\n",
    "    모델을 테스트 하는 함수\n",
    "\n",
    "    Args:\n",
    "        model : 테스트 모델,\n",
    "        loss_function : 손실함수\n",
    "        test_loader : 테스트 데이터셋,\n",
    "        device : 테스트를 수행할 디바이스\n",
    "\n",
    "    Returns:\n",
    "        학습 오차율, 정확도(%)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval() # 👈 모델을 평가 모드로 설정\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # 👈 기울기 계산 비활성화\n",
    "        running_loss = 0.0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return (running_loss, accuracy)\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, loss_function, train_loader, test_loader, device):\n",
    "  \"\"\"\n",
    "  모델을 훈련 하는 함수\n",
    "\n",
    "  Args:\n",
    "      model : 테스트 모델,\n",
    "      optimizer : 옵티마이저,\n",
    "      loss_function : 손실함수,\n",
    "      train_loader : 훈련 데이터셋\n",
    "      test_loader : 테스트 데이터셋,\n",
    "      device : 훈련을 수행할 디바이스\n",
    "\n",
    "  Returns:\n",
    "      list : 각 에포크마다 학습 오차율, 정확도(%)\n",
    "  \"\"\"\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  print(\"Starting Training...\")\n",
    "  num_epochs = 5  # 👈 전체 데이터셋을 5번 반복 학습 (시간이 오래 걸릴 수 있습니다)\n",
    "  result = []\n",
    "  for epoch in range(num_epochs):\n",
    "      # Dropout, BatchNorm 등이 학습 모드로 동작합니다.\n",
    "      model.train()\n",
    "\n",
    "      # train_loader에서 배치(batch) 단위로 데이터를 가져옵니다.\n",
    "      for i, (images, labels) in enumerate(train_loader):\n",
    "          # 1. 데이터를 디바이스로 이동\n",
    "          images = images.to(device, non_blocking=True)\n",
    "          labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "          # 2. (필수) 옵티마이저의 기울기(gradient)를 0으로 초기화\n",
    "          optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "          # 3. 순전파(Forward pass): 모델에 이미지를 입력하여 출력(로짓) 계산\n",
    "          outputs = model(images)\n",
    "\n",
    "          # 4. 손실(Loss) 계산\n",
    "          loss = loss_function(outputs, labels)\n",
    "\n",
    "          # 5. (필수) 역전파(Backward pass): 손실에 대한 기울기 계산\n",
    "          loss.backward()\n",
    "\n",
    "          # 6. (필수) 옵티마이저 실행: 계산된 기울기를 바탕으로 가중치 업데이트\n",
    "          optimizer.step()\n",
    "\n",
    "      # --- 한 Epoch이 끝날 때마다 테스트 정확도 평가 ---\n",
    "      epoch_loss, epoch_accuracy = test_model(model, loss_function, test_loader, device)\n",
    "      result.append((epoch_loss, epoch_accuracy))\n",
    "      print(f\"에포크 {epoch+1} 학습 완료, 정확도 : {epoch_accuracy}, 손실 : {epoch_loss}\")\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "def make_loss_accuracy_graph(losses:list[tuple[str, list[float]]], accrucies:list[tuple[str, float]]):\n",
    "    \"\"\"\n",
    "    손실, 정확도 그래프를 그려주는 함수\n",
    "\n",
    "    Args:\n",
    "        losses : (훈련조건, 훈련결과) 를 담은 리스트\n",
    "        accrucies : (훈련조건, 훈련결과) 를 담은 리스트\n",
    "\n",
    "    Returns:\n",
    "        없음\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "    # 손실그래프 생성\n",
    "    for loss in losses:\n",
    "        label, values = loss\n",
    "        axes[0].plot([i for i in range (1, len(values)+1)], values, label=label)\n",
    "\n",
    "    axes[0].set_title(\"loss graph\")\n",
    "    axes[0].set_xlabel(\"epoch\")\n",
    "    axes[0].set_ylabel(\"loss\")\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "    axes[0].legend()\n",
    "\n",
    "\n",
    "    # 정확도 그래프 생성\n",
    "    for accuracy in accrucies:\n",
    "        label, values = accuracy\n",
    "        axes[1].plot([i for i in range (1, len(values)+1)], values, label=label)\n",
    "\n",
    "    axes[1].set_title(\"accuracy graph\")\n",
    "    axes[1].set_xlabel(\"epoch\")\n",
    "    axes[1].set_ylabel(\"accuracy(%)\")\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "    axes[1].legend()\n",
    "\n",
    "    fig.tight_layout() # 서브플롯들이 겹치지 않도록 간격조절\n",
    "    plt.show()\n",
    "\n",
    "# 이미지 출력하기\n",
    "def print_img(data, label):\n",
    "    img_grid = data * 0.5 + 0.5\n",
    "    np_grid = img_grid.numpy()\n",
    "    np_grid_rgb = np.transpose(np_grid,(1,2,0)) # 기존 (채널, 높이, 너비) -> (높이, 너비, 채널)\n",
    "\n",
    "    print(label)\n",
    "\n",
    "    plt.figure(figsize=(10, 2)) # 가로 10, 세로 2 인치\n",
    "    plt.imshow(np_grid_rgb)\n",
    "    plt.axis('off') # 축 정보 끄기\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loader, test_loader = get_CIFAR10_data_loader(ratio=0.1)\n",
    "model = get_custom_resnet50(class_count=10)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(\n",
    "#     model.parameters(),\n",
    "#     lr=1e-3,\n",
    "#     weight_decay=1e-4  # 정규화\n",
    "# )\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "no_train = [test_model(model, loss_function, test_loader, device)] * 5\n",
    "train = train_model(model, optimizer, loss_function, train_loader, test_loader, device)\n",
    "\n",
    "# 그래프 생성\n",
    "losses = [(\"no_train\", list(map(lambda t:t[0], no_train))), (\"train\", list(map(lambda t:t[0], train)))]\n",
    "accrucies = [(\"no_train\", list(map(lambda t:t[1], no_train))), (\"train\", list(map(lambda t:t[1], train)))]\n",
    "make_loss_accuracy_graph(losses=losses ,accrucies=accrucies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda3c0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
