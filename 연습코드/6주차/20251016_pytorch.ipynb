{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2dd62f7-1a6d-41e4-9732-28e293ddec0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# PyTorch in One Hour: \n",
    "\n",
    "- https://sebastianraschka.com/teaching/pytorch-1h/\n",
    "\n",
    "This tutorial covers the following topics:\n",
    "\n",
    "- An overview of the PyTorch deep learning library\n",
    "- Setting up an environment and workspace for deep learning\n",
    "- Tensors as a fundamental data structure for deep learning\n",
    "- The mechanics of training deep neural networks\n",
    "- Training models on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5d98e-f813-49bc-a270-264950ac533c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The three core components of PyTorch\n",
    "\n",
    "![fig_1](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_01.webp)\n",
    "Figure 1. PyTorch's three main components include a tensor library as a fundamental building block for computing, automatic differentiation for model optimization, and deep learning utility functions, making it easier to implement and train deep neural network models.\n",
    "\n",
    "- PyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\n",
    "\n",
    "- PyTorch is an automatic differentiation engine, also known as *autograd*, which enables the <u>automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization</u>.\n",
    "\n",
    "- PyTorch is a <u>deep learning library</u>, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267af39f-7a1f-4231-8e75-ac605dc337cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Deep learning\n",
    "\n",
    "Unlike traditional machine learning techniques that excel at simple pattern recognition, deep learning is particularly good at handling unstructured data like images, audio, or text, so deep learning is particularly well suited for LLMs.\n",
    "\n",
    "The typical predictive modeling workflow (also referred to as supervised learning) in machine learning and deep learning is summarized in Figure 2.\n",
    "\n",
    "![fig_2](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_03.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398f04c-69d4-45cb-b3ca-e62bad0e52f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Installing PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab491cc-a03f-4bbe-b680-85a8a99647d5",
   "metadata": {},
   "source": [
    "![fig_3](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_04.webp)\n",
    "Figure 3. Access the PyTorch installation recommendation on https://pytorch.org to customize and select the installation command for your system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44081447-ad7e-458d-8a0c-d43c110960ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c5954-0acd-4e12-8f87-969f1c7f8d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Check GPU avaliablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14a5df3-525b-45db-a63a-c5e398a38439",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nvida gpu\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c95b06-8be8-43e3-9d08-a67d87b5362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple M series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d7cc48-bc19-4308-bdf5-2b21ee878ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0323ee75-e75c-4714-9ca0-5e119b68b687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_built()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4b1c6-1da6-4f6b-806e-fa1abaec0788",
   "metadata": {},
   "source": [
    "|||\n",
    "|-|-|\n",
    "| Output | Meaning | \n",
    "| True / True | MPS backend is built and available — Metal acceleration is usable.|\n",
    "| True / False | Rare — indicates partial support. Usually, PyTorch was not compiled with MPS.|\n",
    "| False / True | PyTorch has MPS compiled in, but it can’t access your GPU right now (e.g. running in a virtual env without GPU access).\n",
    "| False / False | MPS not supported or using PyTorch built without Metal backend.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644f723-1a13-4452-a202-6ce2438c667f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Understanding tensors\n",
    "\n",
    "- Tensors represent a mathematical concept that generalizes vectors and matrices to potentially higher dimensions. \n",
    "- Tensors are mathematical objects that can be characterized by their order (or rank), which provides the number of dimensions.\n",
    "- For example,\n",
    "    - a scalar (just a number) is a tensor of rank 0\n",
    "    - a vector is a tensor of rank 1\n",
    "    - a matrix is a tensor of rank 2, as illustrated in Figure 6.\n",
    "\n",
    "![fig_4](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_06.webp)\n",
    "Figure 4. An illustration of tensors with different ranks. Here 0D corresponds to rank 0, 1D to rank 1, and 2D to rank 2. Note that a 3D vector, which consists of 3 elements, is still a rank 1 tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046237ee-bb69-4879-9823-ccf7d6b0f1f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Scalars, vectors, matrices, and tensors\n",
    "\n",
    "- PyTorch tensors are data containers for array-like structures.\n",
    "- There is no specific term for higher-dimensional tensors, so we typically refer to a 3-dimensional tensor as just a 3D tensor, and so forth.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea9f485-f821-4eee-97fc-fe083e152c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) <class 'torch.Tensor'> torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# create a 0D tensor (scalar) from a Python integer\n",
    "tensor0d = torch.tensor(1)\n",
    "print(tensor0d, type(tensor0d), tensor0d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be8da93-4ba9-4e1f-a25c-482d3bd41c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1]) <class 'torch.Tensor'> torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "tensor0d_new = torch.tensor([1])\n",
    "print(tensor0d_new, type(tensor0d_new), tensor0d_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2344ab-da21-4031-8ae4-8334df0a4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) <class 'torch.Tensor'> torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# create a 1D tensor (vector) from a Python list\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d, type(tensor1d), tensor1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33b15d7-766e-4cf2-a8f8-0774a2f9877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]]) <class 'torch.Tensor'> torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# create a 2D tensor from a nested Python list\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
    "print(tensor2d, type(tensor2d), tensor2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae133d0-492c-4a43-988b-9ef2242a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]]) <class 'torch.Tensor'> torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# create a 3D tensor from a nested Python list\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(tensor3d, type(tensor3d), tensor3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fa2097-36aa-4272-b27b-8c4fb49c29f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1,  2],\n",
      "          [ 3,  4]],\n",
      "\n",
      "         [[ 5,  6],\n",
      "          [ 7,  8]]],\n",
      "\n",
      "\n",
      "        [[[ 9, 10],\n",
      "          [11, 12]],\n",
      "\n",
      "         [[13, 14],\n",
      "          [15, 16]]]]) <class 'torch.Tensor'> torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tensor4d = torch.tensor( np.arange(1, 17).reshape(2, 2, 2, 2) )\n",
    "print(tensor4d, type(tensor4d), tensor4d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f04bae-6e14-42d5-9573-19938a50cedf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Tensor data types\n",
    "\n",
    "- In this case, PyTorch adopts the default 64-bit integer data type from Python.\n",
    "- We can access the data type of a tensor via the `.dtype` attribute of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a6eac5-c3c8-480e-aead-a428a32da418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65360968-b99a-46ee-b533-d6e41ea009cf",
   "metadata": {},
   "source": [
    "- If tensors from Python floats, PyTorch creates tensors with a 32-bit precision by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d189b0-7bbc-4500-9373-18b4d599d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1c878-e4b1-4fec-8d7e-5f13e4e01946",
   "metadata": {},
   "source": [
    "- This choice is primarily due to the balance between precision and computational efficiency.\n",
    "- A 32-bit floating point number offers sufficient precision for most deep learning tasks, while consuming less memory and computational resources than a 64-bit floating point number.\n",
    "- Moreover, GPU architectures are optimized for 32-bit computations, and using this data type can significantly speed up model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5668f-99d5-4fd3-b198-ee6106cf359f",
   "metadata": {},
   "source": [
    "It is possible to readily change the precision using a tensor’s `.to` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a2d155-b072-4303-96de-6c6b0139f3d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a91fc85-0261-465f-b794-4d55f6bcee5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -2.],\n",
       "        [ 3.,  4.]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -2.], [3., 4.]], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f1e943-0917-4678-ad60-53cc7214da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = x.pow(2).sum() # x^2\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acf19f68-6af0-4704-b8c9-8d97f12e12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4b8f7b-1ec6-48f4-b9a4-fae6cdf524f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -4.],\n",
       "        [ 6.,  8.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cff26-6407-4d4f-be61-812512ec3c16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Common PyTorch tensor operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d52088-eadd-48a9-89a0-0f5a6393d68b",
   "metadata": {},
   "source": [
    "We already introduced the `torch.tensor()` function to create new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "694877dc-7caf-4ad8-a72b-5494bf24e0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "tensor2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bbeaba7-2b57-4094-b249-6bf48dcf5d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae9271d-089c-42fb-a67e-243b9514423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.reshape(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3425d-5cdc-47a1-ace7-88483bfd5e5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "However, note that the more common command for reshaping tensors in PyTorch is `.view()`:\n",
    "\n",
    "> Most traditional PyTorch code and tutorials use `.view()` because it was introduced first and is slightly faster for contiguous tensors. However, `.reshape()` is more versatile for general use, especially as PyTorch evolves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c30b1f-2ae4-4a06-9bb8-60c7fd1941a5",
   "metadata": {},
   "source": [
    "Next, we can use `.T` to transpose a tensor, which means flipping it across its diagonal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a98f178-0d9e-41d8-84e0-9d2b4f96c0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631815e-ca45-46f3-96ad-41e4567c9284",
   "metadata": {},
   "source": [
    "Lastly, the common way to multiply two matrices in PyTorch is the `.matmul` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7496f8fe-22e5-4aaf-b167-5fd4b8778097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d.matmul(tensor2d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3a65ac3-3993-4519-8ef3-e76cbccdf022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 32],\n",
       "        [32, 77]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2d @ tensor2d.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a9c72-7b29-4119-94ae-00ef7881b1e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Seeing models as computation graphs\n",
    "PyTorch’s autograd system provides functions to compute gradients in dynamic computational graphs automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871e453-96f3-4608-bf12-a158bfe29910",
   "metadata": {},
   "source": [
    "- A computational graph (or computation graph in short) is a directed graph that allows us to express and visualize mathematical expressions.\n",
    "- In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network\n",
    "- We will need this later to compute the required gradients for backpropagation, which is the main training algorithm for neural networks.\n",
    "\n",
    "The following code implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network, returning a score between 0 and 1 that is compared to the true class label (0 or 1) when computing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fe36e4-cc1c-496a-9a8e-548684548b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4200])\n",
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])  # true label\n",
    "x1 = torch.tensor([1.1]) # input feature\n",
    "w1 = torch.tensor([2.2]) # weight parameter\n",
    "b = torch.tensor([0.0])  # bias unit\n",
    "\n",
    "z = x1 * w1 + b          # net input\n",
    "print(z)\n",
    "a = torch.sigmoid(z)     # activation & output\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde7388-657c-4316-a054-e67f91055d42",
   "metadata": {},
   "source": [
    "- $a = \\frac{1}{1 + e^{-z}}$\n",
    "- $ \\text{BCE}(a, y) = -[y \\cdot \\log(a) + (1-y) \\cdot \\log(1-a)] $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b83d2-6d24-4c24-8d7b-e0d4c21deb4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The point of this example is not to implement a logistic regression classifier but rather to illustrate how we can think of a sequence of computations as a computation graph.\n",
    "\n",
    "![fig_5](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_07.webp)\n",
    "A logistic regression forward pass as a computation graph. The input feature `x1` is multiplied by a model weight `w1` and passed through an activation function *σ* after adding the bias. The loss is computed by comparing the model output `a` with a given label `y`.\n",
    "\n",
    "In fact, PyTorch builds such a computation graph in the background, and we can use this to calculate gradients of a loss function with respect to the model parameters (here `w1` and `b`) to train the model, which is the topic of the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37991d85-0d8f-45c2-a2ab-9010538fca6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Automatic differentiation made easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089e134-9239-4a96-bbcf-bde18f05678f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "If we carry out computations in PyTorch, it will build such a graph internally by default if one of its terminal nodes has the `requires_grad` attribute set to `True`. \n",
    "\n",
    "Gradients are required when training neural networks via the popular <u>backpropagation algorithm</u>, which can be thought of as an implementation of the chain rule from calculus for neural networks\n",
    "\n",
    "![fig_6](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_08.webp)\n",
    "\n",
    "Partial derivatives and gradients. \n",
    "\n",
    "- Figure shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables.\n",
    "- A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.\n",
    "- On a high level, the chain rule is a way to compute gradients of a loss function with respect to the model’s parameters in a computation graph.\n",
    "- This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model’s performance, using a method such as gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f5007-d6b9-4f4d-9eb0-d3ced86c653e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "By tracking every operation performed on tensors, PyTorch’s autograd engine constructs a computational graph in the background. Then, calling the grad function, we can compute the gradient of the loss with respect to model parameter `w1` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bdbbf36-d593-40c0-8001-a4721ef9c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "\n",
    "# w1 = torch.tensor([2.2], requires_grad=False)\n",
    "# b = torch.tensor([0.0], requires_grad=False)\n",
    "\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
    "grad_L_b = grad(loss, b, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7243ac-2b5b-4bd6-97f3-498437a9c6a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "By default, PyTorch destroys the computation graph after calculating the gradients to free memory. However, since we are going to reuse this computation graph shortly, we set `retain_graph=True` so that it stays in memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf388f0-db18-4928-a635-7fb93cfdbcd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    }
   ],
   "source": [
    "print(grad_L_w1)\n",
    "print(grad_L_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbe935-5876-4276-801e-9ebbf3dba1ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Above, we have been using the grad function “manually,” which can be useful for experimentation, debugging, and demonstrating concepts. \n",
    "\n",
    "But in practice, PyTorch provides even more high-level tools to automate this process. \n",
    "\n",
    "For instance, we can call `.backward()` on the loss, and PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensors’ `.grad` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34788afa-7c1d-47ed-931b-b0db5ccc4148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50d87c-aa52-4418-9c6e-656dfe6c7798",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Implementing multilayer neural networks\n",
    "\n",
    "An illustration of a multilayer perceptron with 2 hidden layers. Each node represents a unit in the respective layer. Each layer has only a very small number of nodes for illustration purposes.\n",
    "\n",
    "![fig_7](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_09.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61beab-40c8-4727-966e-cb5eabb3c0a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "When implementing a neural network in PyTorch, we typically subclass the `torch.nn.Module` class to define our own custom network architecture. This `Module` base class provides a lot of functionality, making it easier to build and train models. For instance, it allows us to encapsulate layers and operations and keep track of the model’s parameters.\n",
    "\n",
    "Within this subclass, \n",
    "- we define the network layers in the `__init__` constructor and specify how they interact in the `forward` method.\n",
    "- The `forward` method describes how the input data passes through the network and comes together as a computation graph.\n",
    "- In contrast, the `backward` method, which we typically do not need to implement ourselves, is used during training to compute gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "The following code implements a classic multilayer perceptron with two hidden layers to illustrate a typical usage of the `Module` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854181f1-a2e4-4910-ad0a-f565670f88bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf9604a-09e6-4bc9-b6bb-4ddbb607fd7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93b4a6a-0f36-4889-bc85-34e419dcfb86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1536,  0.0858,  0.3405],\n",
      "        [-0.1462, -0.0495,  0.3382],\n",
      "        [-0.1153,  0.0292,  0.2675],\n",
      "        [-0.1191, -0.2425,  0.2436],\n",
      "        [-0.0715,  0.0565,  0.1399]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model(num_inputs, num_outputs):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(num_inputs, 30),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(30, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, num_outputs)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "num_inputs = 50\n",
    "num_outputs = 3\n",
    "model_f = create_model(num_inputs, num_outputs)\n",
    "\n",
    "\n",
    "x = torch.randn(5, num_inputs)\n",
    "\n",
    "logits = model_f(x)\n",
    "\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb944b9-fa5c-4f70-be04-f5be8cb2a0b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can then instantiate a new neural network object as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da31a997-19ba-4a43-8537-d0013a8f96e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(50, 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "679a6445-7ed9-4845-9a5e-c87ea40a0924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_f = create_model(num_inputs, num_outputs)\n",
    "model_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792add03-ad3b-47b1-8f5f-ea3bc71e6486",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Note that we used the `Sequential` class when we implemented the `NeuralNetwork` class. \n",
    "\n",
    "Using `Sequential` is not required, but it can make our life easier if we have a series of layers that we want to execute in a specific order, as is the case here. \n",
    "\n",
    "This way, after instantiating `self.layers = Sequential(...)` in the `__init__` constructor, we just have to call the `self.layers` instead of calling each layer individually in the `NeuralNetwork`’s forward method.\n",
    "\n",
    "Next, let’s check the total number of trainable parameters of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e2063cf-344e-4394-bc2f-badc0815e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a00ebfb7-d3b0-49bc-a06d-dec8b902101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(\n",
    "    p.numel() for p in model_f.parameters() if p.requires_grad\n",
    ")\n",
    "print(\"Total number of trainable model parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b70c98e-cd2c-4cce-9b1e-e0ea33c031eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(50 * 30 + 30) + (30 * 20 + 20) + (20 * 3 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d410c108-4d37-40da-85c8-9a4e68b08979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "30\n",
      "600\n",
      "20\n",
      "60\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bc01f-ec86-47f4-8735-875102484661",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Note that each parameter for which `requires_grad=True` counts as a trainable parameter and will be updated during training.\n",
    "\n",
    "In the case of our neural network model with the two hidden layers above, these trainable parameters are contained in the torch.nn.Linear layers. A linear layer multiplies the inputs with a weight matrix and adds a bias vector. This is sometimes also referred to as a ***feedforward*** or ***fully connected layer***.\n",
    "\n",
    "Based on the `print(model)` call we executed above, we can see that the first Linear layer is at index position 0 in the layers attribute. We can access the corresponding weight parameter matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b85f5594-6de6-4a47-a14f-c107d74d48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0189,  0.0539,  0.0336,  ...,  0.0314,  0.1205, -0.1234],\n",
      "        [ 0.0750,  0.0147, -0.0704,  ..., -0.0581,  0.0733, -0.0049],\n",
      "        [ 0.0690,  0.1288,  0.0683,  ..., -0.1351, -0.0269, -0.1198],\n",
      "        ...,\n",
      "        [ 0.0771,  0.0759,  0.0426,  ...,  0.0330, -0.0856,  0.1361],\n",
      "        [-0.1026, -0.0767,  0.1187,  ..., -0.1289, -0.0432, -0.0453],\n",
      "        [ 0.0311,  0.1265, -0.1335,  ...,  0.0178,  0.0390,  0.0291]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a7a8bff-5b0d-44fc-93b4-75354f60a879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0926, -0.0136,  0.0910, -0.0180,  0.0377, -0.0586, -0.0292, -0.0024,\n",
       "        -0.0862, -0.0338, -0.0357,  0.0130, -0.0719,  0.0206,  0.0377,  0.1295,\n",
       "         0.0698, -0.0370, -0.0904,  0.0012, -0.1218, -0.0387, -0.0920, -0.0168,\n",
       "         0.0341,  0.0153,  0.1381,  0.1095,  0.0670,  0.1255],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dde1558-3c83-4af4-b451-e92bccd6e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c3d908d-da43-4705-b95c-9d447ffbcec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71f3fd-628c-4a3c-a3dc-4a80413f1f9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- The weight matrix above is a 30x50 matrix\n",
    "- We can see that the `requires_grad` is set to `True`, which means its entries are trainable\n",
    "- This is the default setting for weights and biases in `torch.nn.Linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6a632-76aa-49cf-ace2-1050530b8a68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In deep learning, initializing model weights with small random numbers is desired to break symmetry during training – otherwise, the nodes would be just performing the same operations and updates during backpropagation, which would not allow the network to learn complex mappings from inputs to outputs.\n",
    "\n",
    "However, while we want to keep using small random numbers as initial values for our layer weights, we can make the random number initialization reproducible by seeding PyTorch’s random number generator via `manual_seed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ea096f-a197-4ffa-895a-cbd15a35920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2d1f960-cc4b-4e15-9663-eb59bf077934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_f = create_model(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ccae9-c566-44d9-9c11-ea9376bcb704",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now, after we spent some time inspecting the `NeuralNetwork` instance, let’s briefly see how it’s used via the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65ecf761-549a-44a6-99aa-2fd4328ad1be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "X = torch.rand((1, 50))\n",
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39531464-29b1-421f-9649-d2302b9172ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "X = torch.rand((1, 50))\n",
    "out = model_f(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd27c8-ac72-40fc-a02c-3fd3c4c15d64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the code above, \n",
    "\n",
    "we generated a single random training example X as a toy input (note that our network expects 50-dimensional feature vectors) and fed it to the model, returning three scores. When we call `model(x)`, it will automatically execute the forward pass of the model.\n",
    "\n",
    "The forward pass refers to calculating output tensors from input tensors. This involves passing the input data through all the neural network layers, starting from the input layer, through hidden layers, and finally to the output layer.\n",
    "\n",
    "These three numbers returned above correspond to a score assigned to each of the three output nodes. Notice that the output tensor also includes a `grad_fn` value.\n",
    "\n",
    "Here, `grad_fn=<AddmmBackward0>` represents the last-used function to compute a variable in the computational graph. In particular, `grad_fn=<AddmmBackward0>` means that the tensor we are inspecting was created via a matrix multiplication and addition operation. PyTorch will use this information when it computes gradients during backpropagation. The `<AddmmBackward0>` part of `grad_fn=<AddmmBackward0>` specifies the operation that was performed. In this case, it is an `Addmm` operation. `Addmm` stands for <u>matrix multiplication (mm) followed by an addition (Add)</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a3f18-8319-4a9b-b063-5ac86fb1e922",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we just want to use a network without training or backpropagation, for example, if we use it for prediction after training, constructing this computational graph for backpropagation can be wasteful as it performs unnecessary computations and consumes additional memory. \n",
    "\n",
    "So, when we <u>use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the `torch.no_grad()` context manager</u>, as shown below. This tells PyTorch that it doesn’t need to keep track of the gradients, which can result in significant savings in memory and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01f04825-3eb1-421d-b25f-744cd368e56e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f60b389-07f5-480f-9646-683b4c56835d",
   "metadata": {},
   "source": [
    "In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (`logits`) without passing them to a nonlinear activation function. \n",
    "\n",
    "That’s because PyTorch’s commonly used loss functions combine the softmax (or sigmoid for binary classification) operation with the negative log-likelihood loss in a single class.\n",
    "\n",
    "The reason for this is numerical efficiency and stability. So, if we want to compute class-membership probabilities for our predictions, we have to call the softmax function explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37a329fb-ea9f-407b-a769-9789e965fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0802f24f-c869-4074-a675-832bc631f727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2942af3-ac9d-4df1-b186-83f866524e97",
   "metadata": {},
   "source": [
    "The values can now be interpreted as class-membership probabilities that sum up to 1. The values are roughly equal for this random input, which is expected for a randomly initialized model without training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a89b24-929e-4a25-8d0a-c7e76c5a47e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7f765-9dbf-410d-ab91-4e797eb71c30",
   "metadata": {},
   "source": [
    "![fig_8](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_10.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00853b8f-62e3-4763-b629-ee498a413245",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We will implement a custom Dataset class that we will use to create a training and a test dataset that we’ll then use to create the data loaders.\n",
    "\n",
    "Let’s start by creating a simple toy dataset of five training examples with two features each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f05289d-3740-43f8-a45f-90958a12bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8d17d4b-6ac9-41bd-b5c8-6d595937bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8a1a9-c36c-4350-9221-0d38fa9c7ce0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "if we have class labels 0, 1, 2, 3, and 4, the neural network output layer should consist of 5 nodes\n",
    "\n",
    "Next, we create a custom dataset class, `ToyDataset`, by subclassing from PyTorch’s `Dataset` parent class, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ca0e143-989a-479e-8ec9-ebfd8bdab9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134ad17-d53a-44fc-a0bc-1044f195441a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This custom ToyDataset class’s purpose is to use it to instantiate a PyTorch DataLoader. But before we get to this step, let’s briefly go over the general structure of the ToyDataset code.\n",
    "\n",
    "In PyTorch, the three main components of a custom Dataset class are \n",
    "- the `__init__` constructor, the `__getitem__` method, and the `__len__` method\n",
    "\n",
    "In the `__init__` method, \n",
    "- we set up attributes that we can access later in the `__getitem__` and `__len__` methods.\n",
    "- This could be file paths, file objects, database connectors, and so on.\n",
    "- Since we created a tensor dataset that sits in memory, we are simply assigning X and y to these attributes, which are placeholders for our tensor objects.\n",
    "\n",
    "In the `__getitem__` method, \n",
    "- we define instructions for returning exactly one item from the dataset via an index.\n",
    "- This means the features and the class label corresponding to a single training example or test instance.\n",
    "- The data loader will provide this index, which we will cover shortly.\n",
    "\n",
    "Finally, `the __len__` method,\n",
    "- Contains instructions for retrieving the length of the dataset.\n",
    "- Here, we use the `.shape` attribute of a tensor to return the number of rows in the feature array.\n",
    "- In the case of the training dataset, we have five rows, which we can double-check as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d3d2a51-dbf9-4480-9cb0-f925f8b8a80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de066fba-4d16-484e-99af-0ebb40a80308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_ds.features), type(train_ds.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffe58589-c4a3-4db2-a89d-f8af6cfb9312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2000,  3.1000],\n",
       "         [-0.9000,  2.9000],\n",
       "         [-0.5000,  2.6000],\n",
       "         [ 2.3000, -1.1000],\n",
       "         [ 2.7000, -1.5000]]),\n",
       " tensor([0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features, train_ds.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13215a7-cfe1-4023-948e-b7286c51a7e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now that we defined a PyTorch `Dataset` class we can use for our toy dataset, we can use PyTorch’s `DataLoader` class to sample from it, as shown in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "174da6e4-1199-4115-b956-4f29af799373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e15b4485-3ac3-47b2-bec4-c91eeda16073",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c114eeb-ea1e-4759-ba2c-cc9a15c5def3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "After instantiating the training data loader, we can iterate over it as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25846c6e-ff76-44e1-9d35-3352f1c31199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #1\n",
      "x: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]),\n",
      "y: tensor([1, 0])\n",
      "\n",
      "Batch #2\n",
      "x: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]),\n",
      "y: tensor([0, 0])\n",
      "\n",
      "Batch #3\n",
      "x: tensor([[ 2.7000, -1.5000]]),\n",
      "y: tensor([1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# batch_size=2, \n",
    "# shuffle=True,\n",
    "\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch #{idx+1}\\nx: {x},\\ny: {y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e62e08-8b03-4a09-832e-411e83855561",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As we can see based on the output above, the train_loader iterates over the training dataset visiting each training example exactly once. This is known as a training ***epoch***. \n",
    "\n",
    "Since we seeded the random number generator using `torch.manual_seed(123)` above, you should get the exact same shuffling order of training examples as shown above. \n",
    "\n",
    "However <u>if you iterate over the dataset a second time, you will see that the shuffling order will change</u>. This is desired to prevent deep neural networks getting caught in repetitive update cycles during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbffbf-8f61-43bb-bd1c-fa17337fa3a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In practice, having a substantially smaller batch as the last batch in a training epoch can disturb the convergence during training. To prevent this, it’s recommended to set `drop_last=True`, which will drop the last batch in each epoch, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bed1dae3-5d07-4f25-96fd-7cff69b17aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True # drop the last batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fcd26ca-23c1-414e-98e2-746666764885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #1\n",
      "x: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]),\n",
      "y: tensor([0, 0])\n",
      "\n",
      "Batch #2\n",
      "x: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]),\n",
      "y: tensor([1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (x, y) in enumerate(train_loader):\n",
    "    print(f\"Batch #{idx+1}\\nx: {x},\\ny: {y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94972e-7680-4106-aad6-105765b5832c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Lastly, let’s discuss the setting `num_workers=0` in the DataLoader. \n",
    "\n",
    "- This parameter in PyTorch’s DataLoader function is crucial for <u>parallelizing data loading and preprocessing</u>.\n",
    "- When num_workers is set to ***0***, the data loading will be done in the main process and not in separate worker processes.\n",
    "- This might seem unproblematic, but it can lead to significant slowdowns during model training when we train larger networks on a GPU.\n",
    "- This is because instead of focusing solely on the processing of the deep learning model, the CPU must also take time to load and preprocess the data.\n",
    "- As a result, the GPU can sit idle while waiting for the CPU to finish these tasks.\n",
    "\n",
    "In contrast, when `num_workers` is set to ***a number greater than zero***, \n",
    "- multiple worker processes are launched to load data in parallel, freeing the main process to focus on training your model and better utilizing your system’s resources.\n",
    "\n",
    "![fig_9](https://sebastianraschka.com/images/teaching/pytorch-1h/figure_11.webp)\n",
    "Figure. Loading data without multiple workers (setting `num_workers=0`) will create a data loading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left subpanel. If multiple workers are enabled, the data loader can already queue up the next batch in the background as shown in the right subpanel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c03b28-f992-43d2-92e8-aac57daa4fdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we are working with very small datasets, \n",
    "- setting num_workers to 1 or larger may not be necessary since the total training time takes only fractions of a second anyway.\n",
    "- if you are working with tiny datasets or interactive environments such as Jupyter notebooks, increasing num_workers may not provide any noticeable speedup.\n",
    "- They might, in fact, lead to some issues. One potential issue is the overhead of spinning up multiple worker processes, which could take longer than the actual data loading when your dataset is small.\n",
    "- for Jupyter notebooks, setting num_workers to greater than 0 can sometimes lead to issues related to the sharing of resources between different processes, resulting in errors or notebook crashes.\n",
    "\n",
    "Setting `num_workers=4` usually leads to optimal performance on many real-world datasets, but optimal settings depend on your hardware and the code used for loading a training example defined in the Dataset class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a49a6-65f2-4cc1-982b-417ffe135b15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc197f5-8b88-42f2-a894-e6170fc0fef3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We’ve discussed all the requirements for training neural networks: \n",
    "- PyTorch’s tensor library, autograd, the Module API, and efficient data loaders.\n",
    "\n",
    "Let’s now combine all these things and train a neural network on the toy dataset from the previous section. The training code is shown in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "314cdb91-69c0-4aa4-b357-873f1973db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac1f20c8-8fcd-4cd6-9eea-3eb1ad89553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_loss = - (labels * np.log(logits) + (1 - labels) * np.log(1 - logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce54a483-21b6-43a3-8a23-ed6d1c1fa903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels) # Loss function\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # Optional model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f81793-b6ee-432d-b202-3e220880e64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41cff693-1bf5-4b12-94b2-8d233860c69f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As we can see, the loss reaches zero after 3 epochs, a sign that the model converged on the training set. However, before we evaluate the model’s predictions, let’s go over some of the details of the preceding code.\n",
    "\n",
    "First, note that we initialized a model with two inputs and two outputs. \n",
    "- That’s because the toy dataset from the previous section has <u>two input features and two class labels</u> to predict.\n",
    "- We used a ***stochastic gradient descent (SGD) optimizer*** with a ***learning rate (lr)*** of 0.5.\n",
    "- The learning rate is a hyperparameter, meaning it’s a <u>tunable setting that we have to experiment with based on observing the loss</u>.\n",
    "- Ideally, we want to choose a learning rate such that the loss converges after a certain number of epochs – the number of epochs is another hyperparameter to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4153a-2615-4a42-b27c-ff8a6f8a110e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In practice, \n",
    "- we often use a third dataset, a so-called ***validation dataset***, <u>to find the optimal hyperparameter settings</u>.\n",
    "- A ***validation dataset*** is similar to a ***test set***. However, while we only want to use a ***test set*** <u>precisely once to avoid biasing the evaluation</u>, we usually use the ***validation set*** <u>multiple times to tweak the model settings</u>.\n",
    "\n",
    "We also introduced new settings called `model.train()` and `model.eval()`. \n",
    "- As these names imply, these settings are used to put the model into a training and an evaluation mode.\n",
    "- This is necessary for components that behave differently during training and inference, such as ***dropout*** or ***batch normalization*** layers.\n",
    "- Since we don’t have dropout or other components in our NeuralNetwork class that are affected by these settings, using `model.train()` and `model.eval()` is redundant in our code above.\n",
    "- However, it’s best practice to include them anyway to avoid unexpected behaviors when we change the model architecture or reuse the code to train a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49689b1d-54aa-4d74-9567-96a4a1488aa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As discussed earlier, \n",
    "- we pass the ***logits*** directly into the ***cross_entropy loss function***, which will apply the ***softmax function*** internally for efficiency and numerical stability reasons. \n",
    "\n",
    "- Then, calling `loss.backward()` </u>will calculate the gradients in the computation graph that PyTorch constructed in the background</u>. \n",
    "\n",
    "- The `optimizer.step()` </u>method will use the gradients to update the model parameters to minimize the loss</u>. In the case of the ***SGD optimizer***, this <u>means multiplying the gradients with the learning rate and adding the scaled negative gradient to the parameters</u>.\n",
    "\n",
    "- Preventing undesired gradient accumulation. It is important to include an `optimizer.zero_grad()` <u>call in each update round to reset the gradients to zero</u>. Otherwise, the gradients will accumulate, which may be undesired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e840a1-7be3-4b45-b755-b071f1dfa073",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0e4c5-1fa5-4669-acc7-8b80ca28f1c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "After we trained the model, we can use it to make predictions, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb1e4346-3709-4d7a-b628-2103490f1211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0cc87-9aef-4278-afc9-a932fc0cc09e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To obtain the class membership probabilities, we can then use PyTorch’s ***softmax function***, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b81a22e9-7998-4da1-a432-b6fba3b8b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3c5fd-4118-443d-9e9c-1528bf716110",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let’s consider the first row in the code output above. Here, the first value (column) means that the training example has a 99.91% probability of belonging to class 0 and a 0.09% probability of belonging to class 1. (The `set_printoptions` call is used here to make the outputs more legible.)\n",
    "\n",
    "We can convert these values into class labels predictions using PyTorch’s `argmax` function, which <u>returns the index position of the highest value in each row if we set</u> `dim=1` (setting `dim=0` <u>would return the highest value in each column</u>):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6cde1b7-0537-4dc2-9b05-533bf4632c16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(probas, dim=1) # probas\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8a5eb-3b20-4970-91b9-8e53a99e9cd8",
   "metadata": {},
   "source": [
    "Note that it is <u>unnecessary to compute softmax probabilities to obtain the class labels</u>. We could also apply the argmax function to the logits (outputs) directly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e7f4f2e-8063-476f-b2ed-5f80fdeb15a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pridictions = torch.argmax(outputs, dim=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e00c0d-1f53-4be6-a077-5500e500293f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Above, we computed the predicted labels for the training dataset. Since the training dataset is relatively small, we could compare it to the true training labels by eye and see that the model is 100% correct. We can double-check this using the `==` comparison operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82e91c8f-b524-4bc7-a219-5bbb255d0632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1877b5fc-24ef-4ddd-aa2d-88c54c2f3bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(predictions == y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67abdb-a0fe-4930-b9dd-6797ac62ed0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Generalize the computation of the prediction accuracy, let’s implement a `compute_accuracy` function as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b4c8d3d-e967-48d9-9d9f-a9dfb5a14ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854491e-a4c4-420d-a43c-e92bb491d90c",
   "metadata": {},
   "source": [
    "Note that the following `compute_accuracy` function iterates over a data loader to compute the number and fraction of the correct predictions. This is because when we work with large datasets, we typically can only call the model on a small part of the dataset due to memory limitations. \n",
    "\n",
    "The `compute_accuracy` function above is a general method that scales to datasets of arbitrary size since, in each iteration, the dataset chunk that the model receives is the same size as the batch size seen during training.\n",
    "\n",
    "Notice that the internals of the `compute_accuracy` function are similar to what we used before when we converted the logits to the class labels.\n",
    "\n",
    "We can then apply the function to the training as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d4a912c4-a187-4f77-8fe4-626229a0359c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f75e39ba-d1b0-4710-bc4d-5ed030754fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e4b13-c9d6-44f5-9b66-dec8d4ddb3bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04515a03-c9b1-40b8-87bb-4766e19b416c",
   "metadata": {},
   "source": [
    "Here’s the recommended way how we can save and load models in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99ba9f59-7e03-4af3-b7b5-1bb8b0efb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae139a-9e7e-41cf-95a3-45e59d28a3ad",
   "metadata": {},
   "source": [
    "The model’s `state_dict` is a Python dictionary object that maps each layer in the model to its trainable parameters (weights and biases). \n",
    "\n",
    "Note that `\"model.pth\"` is an arbitrary filename for the model file saved to disk. We can give it any name and file ending we like; however, `.pth` and `.pt` are the most common conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1467d-4b55-4054-88b8-d481706a25f6",
   "metadata": {},
   "source": [
    "Once we saved the model, we can restore it from disk as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e77abbe4-c040-464f-a9ad-4fd85ce1d64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(2, 2) # needs to match the original model exactly\n",
    "model.load_state_dict(torch.load('model.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f1300-ece3-4245-8bf5-d59cba06c3d3",
   "metadata": {},
   "source": [
    "The `torch.load(\"model.pth\")` function reads the file `\"model.pth\"` and reconstructs the Python dictionary object containing the model’s parameters while `model.load_state_dict()` applies these parameters to the model, effectively restoring its learned state from when we saved it.\n",
    "\n",
    "Note that the line `model = NeuralNetwork(2, 2)` above is not strictly necessary if you execute this code in the same session where you saved a model. \n",
    "\n",
    "However, I included it here to illustrate that we need an instance of the model in memory to apply the saved parameters. Here, the `NeuralNetwork(2, 2)` architecture needs to match the original saved model exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd7bb4-a15b-4e5c-bf12-c7861cf88dfb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Optimizing training performance with GPUs\n",
    "\n",
    "- https://sebastianraschka.com/teaching/pytorch-1h/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20157ba1-80de-46b4-9011-30f5787ddeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
