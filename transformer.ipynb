{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9x2w0aZxZoCuef0e63q6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marha-hwang/llm_study/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 띄어쓰기 단위로 분리\n",
        "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
        "input_text_list = input_text.split()\n",
        "print(\"input_text_list: \", input_text_list)\n",
        "\n",
        "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
        "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
        "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
        "print(\"str2idx: \", str2idx)\n",
        "print(\"idx2str: \", idx2str)\n",
        "\n",
        "# 토큰을 토큰 아이디로 변환\n",
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(\"input_ids: \", input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8YjynRUO_0u",
        "outputId": "750a8c6a-b073-4b32-ab87-d28be585ad9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
            "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
            "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
            "input_ids:  [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeG0CDbiO0o1",
        "outputId": "0cd54e09-ca86-448a-ef1f-147e1195eaae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.3727, -0.0508,  0.4007, -0.8596, -0.2245, -1.0912,  0.7360,\n",
              "           0.8636,  0.7387, -1.1510, -1.4466,  1.3345,  0.5277, -0.2673,\n",
              "           0.9286, -0.7230],\n",
              "         [ 0.7206, -0.8104,  0.7783,  0.2679, -1.8342, -0.8210,  0.6667,\n",
              "           0.5271, -0.4010, -0.6730,  1.4353, -0.5447,  0.0603,  0.6748,\n",
              "          -0.4917, -2.0448],\n",
              "         [ 0.5705, -1.1236,  2.2578,  1.0732,  0.9077, -0.2606, -1.7578,\n",
              "          -1.1954, -1.3003, -0.0756,  0.5767,  0.8461, -0.5217, -0.8787,\n",
              "           0.7395, -0.2119],\n",
              "         [-1.2143,  0.4815,  0.5606,  0.5548, -0.2083, -0.1828, -0.2618,\n",
              "          -0.3724, -0.7266,  0.0573,  0.2720,  1.0197, -0.6294, -0.7825,\n",
              "          -0.0986,  0.5066],\n",
              "         [ 0.2251,  0.0348,  1.1538, -0.0355,  0.6677,  1.0349,  0.7457,\n",
              "           0.4727, -0.5410, -1.6764,  1.3573,  0.4374, -0.6730,  0.7946,\n",
              "           1.6037, -1.4407]]], grad_fn=<UnsqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 16\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "\n",
        "input_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n",
        "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
        "\n",
        "\n",
        "input_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "max_position = 12\n",
        "# 토큰 임베딩 층 생성\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "# 위치 인코딩 층 생성\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
        "\n",
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "position_encodings = position_embed_layer(position_ids)\n",
        "token_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n",
        "token_embeddings = token_embeddings.unsqueeze(0) # (1, 5, 16)\n",
        "# 토큰 임베딩과 위치 인코딩을 더해 최종 입력 임베딩 생성\n",
        "input_embeddings = token_embeddings + position_encodings\n",
        "input_embeddings.shape"
      ],
      "metadata": {
        "id": "j73axAXNGOjB",
        "outputId": "6aa8648b-569f-47a9-a16b-4c907e1e3b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전코드에서 토큰인코딩과 위치 인코딩값을 활용하여 임베딩값을 생성하였음\n",
        "# embedding_dim을 Linear에 통과시켜 head_dim과 같은 차원으로 변환 시키기 위한 과정\n",
        "# 아직까지는 무작위로 초기화됨\n",
        "# 임베딩 벡터를 보강하여 문맥적의미를 포함하기 위한 과정\n",
        "\n",
        "head_dim = 16\n",
        "\n",
        "# 쿼리, 키, 값을 계산하기 위한 변환\n",
        "weight_q = nn.Linear(embedding_dim, head_dim)\n",
        "weight_k = nn.Linear(embedding_dim, head_dim)\n",
        "weight_v = nn.Linear(embedding_dim, head_dim)\n",
        "# 변환 수행\n",
        "querys = weight_q(input_embeddings) # (1, 5, 16)\n",
        "keys = weight_k(input_embeddings) # (1, 5, 16)\n",
        "values = weight_v(input_embeddings) # (1, 5, 16)"
      ],
      "metadata": {
        "id": "WrOxnYVehJUq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "\tdim_k = querys.size(-1) # 16\n",
        "\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
        "\tweights = F.softmax(scores, dim=-1)\n",
        "\treturn weights @ values"
      ],
      "metadata": {
        "id": "W6JIw4mVpLNn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"원본 입력 형태: \", input_embeddings.shape)\n",
        "\n",
        "after_attention_embeddings = compute_attention(querys, keys, values)\n",
        "\n",
        "print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)\n",
        "print(after_attention_embeddings)\n",
        "# 원본 입력 형태:  torch.Size([1, 5, 16])\n",
        "# 어텐션 적용 후 형태:  torch.Size([1, 5, 16])"
      ],
      "metadata": {
        "id": "fPxfNn-kpL-c",
        "outputId": "6d176f4a-d57e-42fd-c609-34040a5d09d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 입력 형태:  torch.Size([1, 5, 16])\n",
            "어텐션 적용 후 형태:  torch.Size([1, 5, 16])\n",
            "tensor([[[-0.2289, -0.4645, -0.6182, -0.3747, -0.0192,  0.4864, -0.8260,\n",
            "           0.2830,  0.2665, -0.0849, -0.0899,  0.4328, -0.5040,  0.2665,\n",
            "           0.4967, -1.0866],\n",
            "         [-0.2468, -0.2954, -0.4412, -0.3658, -0.1651,  0.1934, -0.7989,\n",
            "           0.2826,  0.3279, -0.1429, -0.0914,  0.5138, -0.4130,  0.3678,\n",
            "           0.4747, -1.2167],\n",
            "         [ 0.2813, -0.6740, -0.1251, -0.4751, -0.0346,  0.4857, -0.6328,\n",
            "           0.0218,  0.2873,  0.0352,  0.0419,  0.0499, -0.1024,  0.0890,\n",
            "           0.5233, -0.1875],\n",
            "         [ 0.2585, -0.4091,  0.2260, -0.3997, -0.2212,  0.0962, -0.6139,\n",
            "           0.0313,  0.4716, -0.0281,  0.0399,  0.0109,  0.0546,  0.1823,\n",
            "           0.4876, -0.2803],\n",
            "         [-0.1863, -0.2750, -0.1128, -0.3057, -0.1687,  0.1570, -0.7412,\n",
            "           0.1900,  0.4724, -0.0942, -0.0521,  0.1806, -0.2304,  0.3027,\n",
            "           0.4737, -0.8491]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n",
        "    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n",
        "    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    outputs = compute_attention(\n",
        "        self.weight_q(querys),  # 쿼리 벡터\n",
        "        self.weight_k(keys),    # 키 벡터\n",
        "        self.weight_v(values),  # 값 벡터\n",
        "        is_causal=self.is_causal\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
        "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"
      ],
      "metadata": {
        "id": "B7cJ_-BppMOY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.n_head = n_head\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
        "    self.concat_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    B, T, C = querys.size()\n",
        "    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    attention = compute_attention(querys, keys, values, self.is_causal)\n",
        "    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    output = self.concat_linear(output)\n",
        "    return output\n",
        "\n",
        "n_head = 4\n",
        "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
        "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
        "after_attention_embeddings.shape"
      ],
      "metadata": {
        "id": "bSw9pwOUpMSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}